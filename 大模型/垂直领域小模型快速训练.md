---
sort: 9
---

# 垂直领域小模型快速训练


* [算法开发手册](https://kg-nlp.github.io/Algorithm-Project-Manual/大模型/垂直领域小模型快速训练.html)

* [个人知乎](https://www.zhihu.com/people/zhangyj-n)

## 背景

服务器资源受限,大模型玩不起,只能继续在1B以下的模型上耕耘。之前写过基于pytorch的增量预训练模型,由于最近在使用百度文心的预训练模型做分类任务,故在百度开源的ERNIE预训练上增加相关任务数据继续训练得到垂直领域模型,用于后续的微调任务

* 之前的分享[算法框架-预训练-增量/全量自定义垂直领域预训练模型](https://zhuanlan.zhihu.com/p/465462642)
* [算法框架-预训练-全词掩码源码分析](https://zhuanlan.zhihu.com/p/469890497)
* 整个增量预训练过程参考[ERNIE中文预训练介绍](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/model_zoo/ernie-1.0/pretraining_introduction.md) 


## 数据准备

* 本次只选取了一本与自身业务相关的pdf电子书进行数据提取,提取方法用到了之前分享中的部分代码[工程框架-文档解析-1-规则解析](https://zhuanlan.zhihu.com/p/640009803),最终将pdf转为txt文本。
* 数据处理参考[PaddleNLP预训练数据流程](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-1.0/preprocess)


|步骤|阶段|数据格式| 样例|
|-|-|-|-|
| 0️⃣初始状态 | -|原始数据： <br/> **每个doc之间用空行间隔开** <br/> - 中文，默认每句换行符，作为句子结束。<br/> - 英文，默认使用nltk判断句子结束  | ```飞桨是功能完备、开源开放的产业级深度学习平台。``` <br/> ```飞桨拥有核心训练和推理框架、基础模型库。``` <br/><br/> ```PaddleNLP是自然语言处理领域的优秀工具。```  |
|1️⃣原始数据转换<br/>`trans_to_json.py`|预处理 <br>输入：0️⃣初始状态 <br>输出：jsonl|jsonl格式：每个doc对应一行json字符串| ```{"text": "飞桨是功能完备、开源开放的产业级深度学习平台。飞桨拥有..."}```<br/>```{"text": "PaddleNLP是自然语言..."}```
|❇️(**可选**)数据中文分词<br/>`words_segmentation.py`|语料分词：中文WWM <br>输入：jsonl  <br> 输出：0️⃣初始状态| 将jsonl格式的数据，恢复成分词后的原始格式数据 <br> | ```飞桨 是 功能 完备、开源 开放的 产业级 深度学习 平台。``` <br/> ```飞桨 拥有 核心 训练和推理 框架、基础 模型库。``` <br/><br/> ```PaddleNLP 是 自然语言处理领域 的 优秀工具。```
|2️⃣数据ID化<br/>`create_pretrain_data.py`|预处理| npy格式：数据id化后的token id <br/>npz格式：数据句子、文章位置索引 | -
|3️⃣训练index文件生成|训练启动|npy格式：<br/> 根据训练步数max_steps生成<br/>train、valid、test的每个样本索引文件| -
|4️⃣token动态mask（可选）| Dataset取数据 | 无 |-


注意：
  * 当你的数据比较少时，分词耗时较少，不需要词步骤。直接在`create_pretrain_data.py`步骤中分词即可。
  * 目的是为了提前分词，加快后续数据ID转化步骤。
  * 如果这里输入的是 jsonl格式文件，最好为多文件，`trans_to_json.py` 时候开启`no-merge`选项。
  * 当你的数据集比较大，或者需要尝试多次转换数据的时候，提前分词可以避免`create_pretrain_data.py`时每次都运行一次分词程序。
* 转换后，需要重新 进行步骤 1️⃣`原始数据转换 trans_to_json.py`，最后2️⃣`数据ID化`步骤设置`--cn_splited=True`参数。
* 2️⃣`数据ID化`也可以在转化ID的同时，一起实现分词。不需要❇️`数据中文分词`步骤。

**本次实际操作过程,依次执行** 
words_segmentation.py  
trans_to_json.py  
create_pretraining_data.py  


> 如果数据较少的话,直接使用trans_to_json.py 和create_pretraining_data.py 即可


* words_segmentation.py     

脚本关键参数
```python
parser.add_argument("--input_path", type=str, default='../data_file/训练文本.txt',required=False, help="Path to you raw files. Folder or file path.")
parser.add_argument("--output_path", type=str, default="../data_file/lac", help="Path to save the output json files.")
```

训练文本.txt格式:
```
飞桨是功能完备、开源开放的产业级深度学习平台。
飞桨拥有核心训练和推理框架、基础模型库。

飞桨是功能完备、开源开放的产业级深度学习平台。
飞桨拥有核心训练和推理框架、基础模型库。
```
使用lac进行分词,分词后的文件 在lac文件目录下

我们要处理成下面的格式,原脚本处理完每句间都有空行,这样就成了多个doc,每个doc只有一句样本,无法使用sop任务。

```
飞桨 是 功能 完备、开源 开放的 产业级 深度学习 平台。
飞桨 拥有 核心 训练和推理 框架、基础 模型库。

飞桨 是 功能 完备、开源 开放的 产业级 深度学习 平台。
飞桨 拥有 核心 训练和推理 框架、基础 模型库。
```

* trans_to_json.py

脚本关键参数

```python
parser.add_argument("--input_path", type=str, default='../data_file/lac/训练文本.txt', help="Path to you raw files. Folder or file path.")
parser.add_argument("--output_path", type=str, default='../data_file/lac/训练文本.jsonl', help="Path to save the output json files.")
```

训练文件.jsonl格式

```
{"text":"飞桨 是 功能 完备、开源 开放的 产业级 深度学习 平台。"}
{"text":"飞桨 拥有 核心 训练和推理 框架、基础 模型库。"}

{"text":"飞桨 是 功能 完备、开源 开放的 产业级 深度学习 平台。"}
{"text":"飞桨 拥有 核心 训练和推理 框架、基础 模型库。"}
```

* create_pretraining_data.py  

脚本关键参数

```python
group.add_argument("--input_path", default='../data_file/lac/训练文本.jsonl', type=str, required=False, help="Path to input JSON files.")
group.add_argument("--output_prefix", default='../data_file/训练文本',type=str, required=False, help="Output prefix to store output file.")
group.add_argument("--split_sentences", default=True, help="Split documents into sentences.")
```
会在data_file下生成两个文件:训练文本.npz和训练文本.npy

最终生成的数据

_ids.npy文件 所有的token_id  

```
all_doc_ids [  108 40835   114 ...   374 40656 12043]  (889415,)
```

_idx.npz  doc 和 文本每句话的长度  

```
lens [ 40 223 175 ... 115 101  68]  (5740,)
docs [   0   19   37   69   91  186  206 ... 16  341  373  426]  (212,)
```

```
Total sentences num: 5740
Total documents num: 211
Total tokens num: 889415
Average tokens per sentence: 154.95
Average tokens per document: 4215.24
```

## 开始训练

* 执行脚本

具体参数设置在执行过程中
```bash
run.sh

#!/usr/bin/env bash
set -x
unset CUDA_VISIBLE_DEVICES

rm -rf core.*

python -u -m paddle.distributed.launch \
    --gpus "0,1,2,3" \
    --log_dir "output/ernie_continue_training/logs" \
    run_pretrain.py \
    --max_steps 200 \
    --checkpoint_steps 50 \
    --save_steps 50 \
    --logging_freq 10 \
    --eval_freq 50 \
```

* 部分参数说明

```
input_dir: 存放npz,npy的目录
split: 按照doc进行切割
binary_head: 启动sop任务
max_steps: 根据总的sentence数量和并行数量和batchsize确定
```


* 执行过程

```bash
[2023-06-29 06:43:11,198] [    INFO] - model_type          :ernie
[2023-06-29 06:43:11,198] [    INFO] - model_name_or_path  :ernie-3.0-medium-zh
[2023-06-29 06:43:11,198] [    INFO] - tokenizer_name_or_path:ernie-3.0-medium-zh
[2023-06-29 06:43:11,198] [    INFO] - continue_training   :True
[2023-06-29 06:43:11,199] [    INFO] - input_dir           :./data_file
[2023-06-29 06:43:11,199] [    INFO] - output_dir          :output/ernie_continue_training/
[2023-06-29 06:43:11,199] [    INFO] - split               :98,1,1
[2023-06-29 06:43:11,199] [    INFO] - binary_head         :True
[2023-06-29 06:43:11,199] [    INFO] - max_seq_len         :256
[2023-06-29 06:43:11,199] [    INFO] - micro_batch_size    :32
[2023-06-29 06:43:11,199] [    INFO] - global_batch_size   :128
[2023-06-29 06:43:11,199] [    INFO] - weight_decay        :0.0
[2023-06-29 06:43:11,199] [    INFO] - grad_clip           :0.0
[2023-06-29 06:43:11,199] [    INFO] - max_lr              :0.0001
[2023-06-29 06:43:11,199] [    INFO] - min_lr              :5e-05
[2023-06-29 06:43:11,199] [    INFO] - warmup_rate         :0.01
[2023-06-29 06:43:11,199] [    INFO] - adam_beta1          :0.9
[2023-06-29 06:43:11,200] [    INFO] - adam_beta2          :0.999
[2023-06-29 06:43:11,200] [    INFO] - adam_epsilon        :1e-08
[2023-06-29 06:43:11,200] [    INFO] - num_train_epochs    :1
[2023-06-29 06:43:11,200] [    INFO] - max_steps           :200
[2023-06-29 06:43:11,200] [    INFO] - checkpoint_steps    :50
[2023-06-29 06:43:11,200] [    INFO] - save_steps          :50
[2023-06-29 06:43:11,200] [    INFO] - decay_steps         :10000
[2023-06-29 06:43:11,200] [    INFO] - logging_freq        :10
[2023-06-29 06:43:11,200] [    INFO] - eval_freq           :50
[2023-06-29 06:43:11,200] [    INFO] - eval_iters          :10
[2023-06-29 06:43:11,200] [    INFO] - use_sharding        :None
[2023-06-29 06:43:11,200] [    INFO] - sharding_degree     :1
[2023-06-29 06:43:11,200] [    INFO] - dp_degree           :4
[2023-06-29 06:43:11,201] [    INFO] - mp_degree           :1
[2023-06-29 06:43:11,201] [    INFO] - pp_degree           :1
[2023-06-29 06:43:11,201] [    INFO] - use_recompute       :None
[2023-06-29 06:43:11,201] [    INFO] - use_amp             :None
[2023-06-29 06:43:11,201] [    INFO] - fp16_opt_level      :O1
[2023-06-29 06:43:11,201] [    INFO] - enable_addto        :True
[2023-06-29 06:43:11,201] [    INFO] - scale_loss          :1024
[2023-06-29 06:43:11,201] [    INFO] - hidden_dropout_prob :0.1
[2023-06-29 06:43:11,201] [    INFO] - attention_probs_dropout_prob:0.1
[2023-06-29 06:43:11,201] [    INFO] - seed                :1234
[2023-06-29 06:43:11,201] [    INFO] - num_workers         :2
[2023-06-29 06:43:11,201] [    INFO] - check_accuracy      :None
[2023-06-29 06:43:11,202] [    INFO] - device              :gpu
[2023-06-29 06:43:11,202] [    INFO] - lr_decay_style      :cosine
[2023-06-29 06:43:11,202] [    INFO] - share_folder        :None
[2023-06-29 06:43:11,202] [    INFO] - masked_lm_prob      :0.15
[2023-06-29 06:43:11,202] [    INFO] - short_seq_prob      :0.1
[2023-06-29 06:43:11,202] [    INFO] - favor_longer_ngram  :False
[2023-06-29 06:43:11,202] [    INFO] - max_ngrams          :3
[2023-06-29 06:43:11,202] [    INFO] - test_iters          :100
[2023-06-29 06:43:11,202] [    INFO] - accumulate_steps    :1

> building dataset index ...
 > finished creating indexed dataset in 0.005514 seconds
 > indexed dataset stats:
    number of documents: 211
    number of sentences: 5740
 > dataset split:
    train:
     document indices in [0, 207) total of 207 documents
     sentence indices in [0, 5640) total of 5640 sentences
    validation:
     document indices in [207, 209) total of 2 documents
     sentence indices in [5640, 5688) total of 48 sentences
    test:
     document indices in [209, 211) total of 2 documents
     sentence indices in [5688, 5740) total of 52 sentences
 > WARNING: could not find index map file ./data_file/工艺手册文本块_train_indexmap_25600mns_253msl_0.10ssp_1234s.npy, building the indices on rank 0 ...
int32
 > building sapmles index mapping for train ...
    using uint32 for data mapping...
    using:
     number of documents:            207
     sentences range:                [0, 5640)
     total number of sentences:      5640
     number of epochs:               2147483646
     maximum number of samples:      25600
     maximum sequence length:        253
     minimum sentences num:          2
     short sequence probability:     0.1
     short sequence ration (1/prob): 10
     seed:                           1234
    reached 25600 samples after 11 epochs ...
   number of empty documents: 0
   number of documents with one sentence: 0
   number of documents with long sentences: 3
   will create mapping for 26311 samples

[2023-06-29 06:43:26,911] [    INFO] - train_ds 26311
[2023-06-29 06:43:26,911] [    INFO] - valid_ds 6403
[2023-06-29 06:43:26,911] [    INFO] - test_ds 12813
[2023-06-29 06:43:26,911] [    INFO] - train_dl 205
[2023-06-29 06:43:26,911] [    INFO] - valid_dl 50
[2023-06-29 06:43:26,911] [    INFO] - test_dl 100
```

## 保存节点

数据量太少,保存了4组模型参数,共耗时≈5min


## 加载使用

* 版本

```
paddlenlp==2.5.2 paddlepaddle==2.4.1
&
paddlenlp=2.6.0rc paddlepaddle==2.5.0
```

* 问题

```
File ".......paddlenlp/transformers/auto/modeling.py", line 226, in _get_model_class_from_config
    f"Unable to parse 'architectures' or 'init_class' from {config_file_path}. Also unable to infer model class from 'pretrained_model_name_or_path'"
AttributeError: Unable to parse 'architectures' or 'init_class' from model/model_2000/config.json. Also unable to infer model class from 'pretrained_model_name_or_path'
```
* 定位代码位置  
 199行
```python
# TODO: Refactor into AutoConfig when available
    @classmethod
    def _get_model_class_from_config(cls, pretrained_model_name_or_path, config_file_path):
        with io.open(config_file_path, encoding="utf-8") as f:
            config = json.load(f)
        print('自定义模型配置文件',json.dumps(config,indent=2,ensure_ascii=False))
        # Get class name corresponds to this configuration
        if is_standard_config(config):
            architectures = config["architectures"]
            init_class = architectures.pop() if len(architectures) > 0 else None
        else:
            init_class = config.pop("init_class", None)
        init_class = init_class[:-5] if init_class is not None and init_class.endswith("Model") else init_class
        # print(init_class)
        model_name = None
        if init_class:
            for model_flag, name in MAPPING_NAMES.items():
                if model_flag in init_class:
                    model_name = model_flag + "Model"
                    break
        else:
            # From pretrained_model_name_or_path
            for model_flag, name in MAPPING_NAMES.items():
                if name in pretrained_model_name_or_path.lower():
                    model_name = model_flag + "Model"
                    break
        if model_name is None:
            raise AttributeError(
                f"Unable to parse 'architectures' or 'init_class' from {config_file_path}. Also unable to infer model class from 'pretrained_model_name_or_path'"
            )
```
* 增量训练生成的config文件
```json
{
  "attention_probs_dropout_prob": 0.1,
  "enable_recompute": null,
  "fuse": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 2048,
  "model_type": "ernie",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 0,
  "paddlenlp_version": null,
  "pool_act": "tanh",
  "task_id": 0,
  "task_type_vocab_size": 16,
  "type_vocab_size": 4,
  "use_task_id": true,
  "vocab_size": 40000
}
```

* 源码modeling.py  模型字符匹配范围
paddlenlp/transformers/auto/modeling.py

```
MAPPING_NAMES = OrderedDict(
    [
        # Base model mapping
        ("Albert", "albert"),
        ("BigBird", "bigbird"),
        ("BlenderbotSmall", "blenderbot_small"),
        ("Blenderbot", "blenderbot"),
        ("ChineseCLIP", "chineseclip"),
        ("ChineseBert", "chinesebert"),
        ("ConvBert", "convbert"),
        ("CTRL", "ctrl"),
        ("DistilBert", "distilbert"),
        ("DalleBart", "dallebart"),
        ("Electra", "electra"),
        ("ErnieViL", "ernie_vil"),
        ("ErnieCtm", "ernie_ctm"),
        ("ErnieDoc", "ernie_doc"),
        ("ErnieGen", "ernie_gen"),
        ("ErnieGram", "ernie_gram"),
        ("ErnieLayout", "ernie_layout"),
        ("ErnieM", "ernie_m"),
        ("Ernie", "ernie"),
        ("FNet", "fnet"),
        ("Funnel", "funnel"),
        ("LayoutXLM", "layoutxlm"),
        ("LayoutLMv2", "layoutlmv2"),
        ("LayoutLM", "layoutlm"),
        ("Luke", "luke"),
        ("MBart", "mbart"),
        ("MegatronBert", "megatronbert"),
        ("MobileBert", "mobilebert"),
        ("MPNet", "mpnet"),
        ("NeZha", "nezha"),
        ("Nystromformer", "nystromformer"),
        ("PPMiniLM", "ppminilm"),
        ("ProphetNet", "prophetnet"),
        ("Reformer", "reformer"),
        ("RemBert", "rembert"),
        ("Roberta", "roberta"),
        ("RoFormerv2", "roformerv2"),
        ("RoFormer", "roformer"),
        ("Skep", "skep"),
        ("SqueezeBert", "squeezebert"),
        ("TinyBert", "tinybert"),
        ("UnifiedTransformer", "unified_transformer"),
        ("UNIMO", "unimo"),
        ("XLNet", "xlnet"),
        ("XLM", "xlm"),
        ("GPT", "gpt"),
        ("MT5", "mt5"),
        ("T5", "t5"),
        ("Bert", "bert"),
        ("Bart", "bart"),
        ("GAUAlpha", "gau_alpha"),
        ("CodeGen", "codegen"),
        ("CLIPVision", "clip"),
        ("CLIPText", "clip"),
        ("CLIP", "clip"),
        ("ChineseCLIPVision", "chineseclip"),
        ("ChineseCLIPText", "chineseclip"),
        ("ChineseCLIP", "chineseclip"),
        ("Artist", "artist"),
        ("OPT", "opt"),
        ("Pegasus", "pegasus"),
        ("DPT", "dpt"),
        ("Bit", "bit"),
        ("BlipText", "blip"),
        ("BlipVision", "blip"),
        ("Blip", "blip"),
    ]
)
```

* 原因

```
在于 输入的模型文件中没有'ernie'字符串
导致 `if name in pretrained_model_name_or_path.lower():` 不存在
解决方法:
1. 修改输入的模型文件名,包含ernie, 比如增量生成的模型文件目录为model_2000, 修改为model_ernie_2000
2. 对配置文件config增加architectures字段或init_class字段,满足上面代码的条件

自定义为model_ernie_micro_2400的时候,加载有问题,会识别为ernie_m,导致model(**batch) 时tokens_type_id出问题,改为model_ernie_2400_micro
```


* 对比试验

对比model_ernie_50和ernie-3.0-medium-zh

在之前的多分类多标签的任务上更换模型,其余参数不变,模型提升效果不明显  
后续还需要增加大量的业务数据进行垂直训练