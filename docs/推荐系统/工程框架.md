---
sort: 1
---


# 工程框架


* [算法开发手册](https://kg-nlp.github.io/Algorithm-Project-Manual/推荐系统/工程框架.html)

* [个人知乎](https://www.zhihu.com/people/zhangyj-n)

* [算法框架-文本匹配-算法汇总（持续更新）](https://zhuanlan.zhihu.com/p/465584667)


## 应用示例

* [SENet双塔模型：在推荐领域召回粗排的应用及其它](https://zhuanlan.zhihu.com/p/358779957)
* [FAQ_system](https://github.com/wzzzd/FAQ_system)
* [faq-qa-sys](https://github.com/lerry-lee/faq-qa-sys)
* [Chatbot_CN](https://github.com/charlesXu86/Chatbot_CN)
* [无监督智能检索问答系统](https://github.com/PaddlePaddle/PaddleNLP/tree/635b272b64485640f4a5e3fe6c79b0846abc6c84/pipelines/examples/unsupervised-question-answering)
* [端到端两路召回语义检索系统](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/pipelines/examples/semantic-search/Multi_Recall.md)
* [手把手搭建一个语义检索系统](https://github.com/PaddlePaddle/PaddleNLP/tree/635b272b64485640f4a5e3fe6c79b0846abc6c84/applications/neural_search)
* [基于FAQ的智能问答(一): Elasticsearch的调教](https://zhuanlan.zhihu.com/p/347957917)
* [基于FAQ的智能问答(二): 召回篇](https://zhuanlan.zhihu.com/p/349993294)
* [基于FAQ的智能问答(三): 精排篇](https://zhuanlan.zhihu.com/p/352316559)
    * 人机对话的几种形式：
        * FAQ-Bot：常见问答对
        * MRC-Bot：机器阅读的智能问答，输入问题，找到文档，再从文档中寻找答案
        * KG-Bot：知识图谱的问答，输入问题，解析图谱查询语句，检索
        * Task-Bot：任务型对话，面对特定场景的多轮对话
        * Chat-Bot：闲聊对话，缺点回复不可控，所以实际落地还是转为FAQ，提前构建一个回复库
    * ES 分词 热更新
    * 使用canal工具 同步sql和es
    * 双路召回，模型处理数据集构造，难例构造
    * [ES同样可以做向量检索](https://www.elastic.co/cn/blog/text-similarity-search-with-vectors-in-elasticsearch)
    * 精排，评价指标使用的是NDCG 归一化折损累计增益
    * 精排数据集构造 
* [推荐系统与大数据框架](https://note.youdao.com/s/1tbVohM9)
    * 视频已经下架,只剩下笔记了 

* [搜索推荐相关技术业务落地方案和源码](https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&__biz=MzkzNDI1Mzc1Nw==&scene=1&album_id=2950313077122646021&count=3#wechat_redirect)

## 无监督智能检索问答系统

* 开发环境

```bash
docker run -it -d --name pytorch_env -p 8085:22  -p 8040:8040 -p 8030:8030 -p 8020:8020 -p 8010:8010 -p 8000:8000 \
-v /home/zhangyj/Algorithm_Frame:/home/Algorithm_Frame \
--shm-size 64g --restart=always -e NVIDIA_VISIBLE_DEVICES=all registry.cn-beijing.aliyuncs.com/sg-gie/pytorch:1.7.0-cuda11.0-cudnn8-devel /bin/bash
```
> 源码路径:  paddleNLP==2.5.2
E:\users\zhangyj-n\EngineeringField\按镜像仓库分类\Algorithm_Frame-本地使用\paddle_pipelines

* 启动ES

```bash
docker run -d --restart=always --name llm_es -p 9200:9200 -p 9300:9300 \
-e "discovery.type=single-node" registry.cn-beijing.aliyuncs.com/sg-gie/es_hot:7.7

检测:curl http://localhost:9200/process_manual/_search
加载语料后再次检测 curl http://localhost:9200/process_manual/_search
```

> **都是pipelines下的案例**

* 进入 /home/Algorithm_Frame/paddle_pipelines
* 创建data目录,放置数据
* 修改 examples/unsupervised-question-answering/offline_question_answer_pairs_generation.py 运行程序,生成离线文档语料库
* 修改 utils/offline_ann.py  运行程序 构建ES ann索引库
* 启动restapi后端  sh examples/unsupervised-question-answering/run_unsupervised_question_answering_server.sh 可以改下端口
* 启动Streamlit WebUI前端 sh examples/unsupervised-question-answering/run_unsupervised_question_answering_web.sh 可以改下端口
* 问题:
    * 由于开发环境未暴露太多端口,所有UI部分无法使用,在启动后端服务后,自定义请求脚本,启动的模型服务也无法在外部访问
* 结果
    * 输出的结果质量不高
    * 预测的原理还需要研究
* 结论
    * 等待好的标注数据:问答对,微调领域模型,包括抽取,生成模型
    * 使用检索式方法返回结果

## 端到端两路召回语义检索系统

```bash
配置文件:paddle_pipelines/rest_api/pipeline/multi_recall_semantic_search.yaml
python utils/offline_ann.py  修改参数
sh examples/semantic-search/run_multi_search_server.sh  修改端口
sh examples/semantic-search/run_multi_search_web.sh  修改端口
```

* 为了使用自定义模型,需要修改pipelines源码

```bash
修改utils/offline_ann.py,
E:\users\zhangyj-n\EngineeringField\按镜像仓库分类\Algorithm_Frame-本地使用\pipelines\nodes\retriever\dense.py
```
> 修改内容

```python
class DensePassageRetriever(BaseRetriever):
   

    def __init__(
        self):
        # Init & Load Encoders
        if os.path.exists(params_path):
            from paddlenlp.data import DataCollatorWithPadding
            pretrained_model = AutoModel.from_pretrained(query_embedding_model)
            self.ernie_dual_encoder = SemanticIndexBatchNeg(pretrained_model, output_emb_size=output_emb_size)
            # print(self.ernie_dual_encoder)
            # Load Custom models
            print("Loading Parameters from:{}".format(params_path))
            state_dict = paddle.load(params_path)
            self.ernie_dual_encoder.set_dict(state_dict)
            self._tokenizer = AutoTokenizer.from_pretrained(query_embedding_model)
            self._collator = DataCollatorWithPadding(self._tokenizer, return_tensors="pd")
            # self.query_tokenizer = AutoTokenizer.from_pretrained(query_embedding_model)
            # self.passage_tokenizer = AutoTokenizer.from_pretrained(query_embedding_model)
            # self.query_encoder = self.passage_encoder =  self.ernie_dual_encoder
        else:
            # print('输入的向量维度: ',output_emb_size)
            self.query_encoder = Taskflow(
                "feature_extraction",
                model=query_embedding_model,
                batch_size=self.batch_size,
                _static_mode=True,
                return_tensors="np",
                max_len=max_seq_len_query,
                output_emb_size=output_emb_size,
                reinitialize=reinitialize,
                share_parameters=share_parameters,
                device_id=0 if use_gpu else -1,
            )
            self.passage_encoder = Taskflow(
                "feature_extraction",
                model=passage_embedding_model,
                batch_size=self.batch_size,
                _static_mode=True,
                return_tensors="np",
                max_len=max_seq_len_passage,
                output_emb_size=output_emb_size,
                reinitialize=reinitialize,
                share_parameters=share_parameters,
                device_id=0 if use_gpu else -1,
            )
    def _get_predictions(self, dicts):
        datasets = []  # 文本列表
        if "passages" in dicts[0]:
            # dicts is a list of passages
            for passages in dicts:
                for item in passages["passages"]:
                    if self.embed_title:
                        datasets.append(item["title"] + item["text"])
                    else:
                        datasets.append(item["text"])
        elif "query" in dicts[0]:
            # dicts is a list of passages
            for passages in dicts:
                datasets.append(passages["query"])

        all_embeddings = {"query": [], "passages": []}

        # When running evaluations etc., we don't want a progress bar for every single query
        if len(datasets) == 1:
            disable_tqdm = True
        else:
            disable_tqdm = not self.progress_bar
        if os.path.exists(self.params_path):
            ''''''
            with tqdm(
                    total=len(datasets) // self.batch_size,
                    unit=" Docs",
                    desc="Create embeddings",
                    position=1,
                    leave=False,
                    disable=disable_tqdm,
            ) as progress_bar:
                for i in range(0, len(datasets), self.batch_size):
                    if "query" in dicts[0]:
                        tokenized_inputs = self._batchify(datasets[i: i + self.batch_size])
                        batch_inputs = self._collator(tokenized_inputs)
                        text_features = self.ernie_dual_encoder.get_pooled_embedding(
                            input_ids=batch_inputs["input_ids"], token_type_ids=batch_inputs["token_type_ids"]
                        )
                        all_embeddings["query"].append(text_features.numpy())
                    if "passages" in dicts[0]:
                        tokenized_inputs = self._batchify(datasets[i: i + self.batch_size])
                        batch_inputs = self._collator(tokenized_inputs)
                        text_features = self.ernie_dual_encoder.get_pooled_embedding(
                            input_ids=batch_inputs["input_ids"], token_type_ids=batch_inputs["token_type_ids"]
                        )
                        all_embeddings["passages"].append(text_features.numpy())
                    progress_bar.update(self.batch_size)
        else:
            with tqdm(
                total=len(datasets) // self.batch_size,
                unit=" Docs",
                desc="Create embeddings",
                position=1,
                leave=False,
                disable=disable_tqdm,
            ) as progress_bar:
                for i in range(0, len(datasets), self.batch_size):

                    if "query" in dicts[0]:
                        cls_embeddings = self.query_encoder(datasets[i : i + self.batch_size])
                        all_embeddings["query"].append(cls_embeddings["features"])
                    if "passages" in dicts[0]:
                        cls_embeddings = self.passage_encoder(datasets[i : i + self.batch_size])
                        all_embeddings["passages"].append(cls_embeddings["features"])
                    progress_bar.update(self.batch_size)
        if all_embeddings["passages"]:
            all_embeddings["passages"] = np.concatenate(all_embeddings["passages"])
        if all_embeddings["query"]:
            all_embeddings["query"] = np.concatenate(all_embeddings["query"])
        # print('all_embeddings["passages"].shape',all_embeddings["passages"].shape)

        return all_embeddings
```


* 在pycharm软件上运行训练,如何使用工程下的目录
    * 先安装pip包  paddle-pipelines,将关联的其他包安装上,再卸载掉paddle-pipelines; 这样修改工程下源码可以加载使用,可以在终端进行shell操作

* 多路召回请求脚本

```python
import json
from typing import Any, Dict, List, Tuple

import requests

API_ENDPOINT = 'http://10.127.91.2:8000'
DOC_REQUEST = "query"


# 多路检索
def multi_recall_semantic_search(
    query, filters={}, top_k_ranker=5, top_k_bm25_retriever=5, top_k_dpr_retriever=5
) -> Tuple[List[Dict[str, Any]], Dict[str, str]]:
    '''
    :param query:  查询问题
    :param filters: 无用
    :param top_k_ranker:  语义召回数量(建议设置小数值)
    :param top_k_bm25_retriever: 字面召回数量(建议设置大数值)
    :param top_k_dpr_retriever: 语义排序返回数量
    :return:
    '''
    url = f"{API_ENDPOINT}/{DOC_REQUEST}"
    params = {
        "filters": filters,
        "DenseRetriever": {"top_k": top_k_dpr_retriever},
        "BMRetriever": {"top_k": top_k_bm25_retriever},
        "Ranker": {"top_k": top_k_ranker},
    }
    req = {"query": query, "params": params}
    response_raw = requests.post(url, json=req)

    if response_raw.status_code >= 400 and response_raw.status_code != 503:
        raise Exception(f"{vars(response_raw)}")

    response = response_raw.json()
    if "errors" in response:
        raise Exception(", ".join(response["errors"]))

    # Format response
    results = []
    answers = response["documents"]
    for answer in answers:
        results.append(
            {
                "context": answer["content"],
                "source": answer["meta"]["name"],
                "answer": answer["meta"]["answer"] if "answer" in answer["meta"].keys() else "",
                "relevance": round(answer["score"] * 100, 2),
                "images": answer["meta"]["images"] if "images" in answer["meta"] else [],
            }
        )
    return results, response

if __name__ == '__main__':
    query_str = '木材表面施涂溶剂型混色涂料施工（装饰工程）的操作工艺'
    # 多路检索
    results, response = multi_recall_semantic_search(query_str, filters={}, top_k_ranker=3, top_k_bm25_retriever=5, top_k_dpr_retriever=3)

    print(json.dumps(results,indent=2,ensure_ascii=False))

    pass
```

## 基于FAQ的智能问答
* 人机对话的几种形式：
    * FAQ-Bot：常见问答对
    * MRC-Bot：机器阅读的智能问答，输入问题，找到文档，再从文档中寻找答案
    * KG-Bot：知识图谱的问答，输入问题，解析图谱查询语句，检索
    * Task-Bot：任务型对话，面对特定场景的多轮对话
    * Chat-Bot：闲聊对话，缺点回复不可控，所以实际落地还是转为FAQ，提前构建一个回复库
* ES 分词 热更新
* 使用canal工具 同步sql和es
* 双路召回，模型处理数据集构造，难例构造
* ES同样可以做向量检索
* 精排，评价指标使用的是NDCG 归一化折损累计增益
* 精排数据集构造

> 参考上面链接  

* 补充网友内容(FAQ数据集清洗的一般步骤)
    * 去除重复项:
        * 包括完全匹配和近似匹配
        * 当数据量上万时,对于近似匹配可以采用基于字面相似度方法,使用编辑距离或简单的使用汉明距离即可
        * 根据自己的业务场景编写正则表达式去除哪些非关键词的相似词,比如"一层楼"/"二层楼","一层"即为非关键信息
        * [算法框架-数据分析-2-大规模文本聚类去重](https://zhuanlan.zhihu.com/p/639758710)
    * 格式一致:在我理解是线下训练和线上预测时,对于数据的前处理方法要一致,存储的格式相同只是为了线下自己训练时方便处理.
    * 去除无关项:如何界定和业务无关
        * 业务专家提供无关项的识别逻辑,提供关键词库
        * 如果逻辑复杂,并且数据量很大,这个时候需要单独上一个检测无关项的二分类模型,标注一批数据进行过滤
        * 比较容易理解的无关数据,算法人员自己写写规则做下数据清洗.
    * 语言清洗: 文本中拼写/语法错误,需要看数据占比,但不好统计,感觉只能是分析数据时看到收集,我认为少的话前期剔除掉即可,如果要上一个拼写语法纠错的模型效果还是难以保证,并且还需要去标注很多这样的数据,公司可能不太会投入资源. 非要弄可以用下LLM,之前简单试过ChatGPT纠错,貌似还行.
        * [工程框架-纠错系统-学习流程](https://zhuanlan.zhihu.com/p/467449183)
    * 数据填充: 通过难例分析来解决了 
        * [算法框架-文本分类-场景实践(一)](https://zhuanlan.zhihu.com/p/646398119)
        * 上面写了点难例分析的角度,还是结合业务场景好理解
    * 数据分割: 这个是个人处理数据习惯吧,不多说了