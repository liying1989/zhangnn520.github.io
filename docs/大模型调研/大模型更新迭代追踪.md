# 													大语言模型追踪

## 一、chatgpt

​		chatgpt训练过程分为三个阶段，分别为训练监督策略模型、训练奖励模型和采用ppo强化学习来优化策略。

![img](https://dc1xi3wyor.feishu.cn/space/api/box/stream/download/asynccode/?code=NmQ0ODNlMDFlZDY2OTMwYTNhMDg1OTA3ZGMyMDNmNDNfeDFrUmlmd0lTa3pZbEFiaERzUnBBVTEwTjJtTzFBalZfVG9rZW46RE1nRWJ4T0Rsb3BSMkN4UTVya2N1QnZBbnNiXzE2OTIxNzM1MTc6MTY5MjE3NzExN19WNA)

### 1.1、**训练监督策略模型**

​		为了让GPT 3.5初步具备理解指令的意图，首先会在数据集中随机抽取问题，由人类标注人员，给出高质量答案，然后用这些人工标注好的数据来微调 GPT-3.5模型（获得SFT模型, Supervised Fine-Tuning）。此时的SFT模型在遵循指令/对话方面已经优于 GPT-3，但不一定符合人类偏好。

![img](https://dc1xi3wyor.feishu.cn/space/api/box/stream/download/asynccode/?code=MjE3N2RjNzkzYjllYjQ1MTc4YWM2MWY1NzIzNDRlOWRfazl0cWhzN25od3U0a2NsUEo1MUtqZ1Fpek9ucm9xdkNfVG9rZW46WE1iV2IxVm1lb2p4OG54ZGJTTmNEMFpabm1iXzE2OTIxNzM1MTc6MTY5MjE3NzExN19WNA)

### 1.2、训练奖励模型（Reward Mode，RM）

​		这个阶段的主要是通过人工标注训练数据（约33K个数据），来训练回报模型。在数据集中随机抽取问题，使用第一阶段生成的模型，对于每个问题，生成多个不同的回答。人类标注者对这些结果综合考虑给出排名顺序。这一过程类似于教练或老师辅导。接下来，使用这个排序结果数据来训练奖励模型。对多个排序结果，两两组合，形成多个训练数据对。RM模型接受一个输入，给出评价回答质量的分数。这样，对于一对训练数据，调节参数使得高质量回答的打分比低质量的打分要高。

![img](https://dc1xi3wyor.feishu.cn/space/api/box/stream/download/asynccode/?code=ZjNhMjM4YWEwZDYwNmRhN2JlZWMzNTNlMzIxODJkYjNfU3pUdWZPelVJTWNCRXNNQ29zVmJGTGJKU2IwSTUza1JfVG9rZW46SUNnNmJiUDVnb0JqQTd4NE80a2NOeWJobnhjXzE2OTIxNzM1MTc6MTY5MjE3NzExN19WNA)

### 1.3、采用PPO（Proximal Policy Optimization，近端策略优化）强化学习来优化策略

​		PPO的核心思路在于将Policy Gradient中On-policy的训练过程转化为Off-policy，即将在线学习转化为离线学习，这个转化过程被称之为Importance Sampling。这一阶段利用第二阶段训练好的奖励模型，靠奖励打分来更新预训练模型参数。在数据集中随机抽取问题，使用PPO模型生成回答，并用上一阶段训练好的RM模型给出质量分数。把回报分数依次传递，由此产生策略梯度，通过强化学习的方式以更新PPO模型参数。如果我们不断重复第二和第三阶段，通过迭代，会训练出更高质量的ChatGPT模型。

![img](https://dc1xi3wyor.feishu.cn/space/api/box/stream/download/asynccode/?code=YTY2ODVmZGFiZDYxN2IzM2EzNmUxMGI4ZDczZjQyNGZfSmNRTHRER3lyb3M0cnlNOGY1THVQVDhDazVmMzJhOUhfVG9rZW46VndJYmJHRFM4b2duUXF4b0tzSWMwZWFjbjVkXzE2OTIxNzM1MTc6MTY5MjE3NzExN19WNA)

## 二、ChatGPT的技术架构

​		ChatGPT之前有几个知名的兄弟，包括GPT-1、GPT-2和GPT-3。这几个兄弟一个比一个个头大，ChatGPT与GPT-3更为相近。

![img](https://dc1xi3wyor.feishu.cn/space/api/box/stream/download/asynccode/?code=NDRmZGYyZmFkMmZhZmUzNGNjYTg1OGYwYjJkNDlmOTZfQjZTNXUxUUxWZHZoVzdLZE82cEt4VU1qUWV3VWg1NmxfVG9rZW46T2l4b2JLOGJBb0pZbWN4Smc4c2Nha3FYbkVjXzE2OTIxNzM1MTc6MTY5MjE3NzExN19WNA)

## 三、大模型模型训练框架

​		大模型在整个训练过程中，SFT微调、RM/RBRM训练和PPO强化学习这三个阶段是相辅相成的。首先，通过SFT微调使模型适应特定任务，然后使用RM或RBRM训练来设计合适的奖励函数，最后应用PPO强化学习算法进一步优化模型策略。这种训练方法的一个关键优势是，它可以将人类专家的知识和偏好引入模型中。通过微调和基于奖励的方法，模型可以学会生成符合人类期望的输出。最后输入的内容需要充分考虑人类的价值观，做到真正理解用户需求、遵循道德和法律规范、适应社会和文化背景和防止有害输出。

### 3.1、基座预训练（Base pretrain）

​		预训练模型的目的是让模型学习到语言的基本结构、语法规则和一般知识，从而为后续的任务定向训练打下基础。在预训练阶段，模型在大量无标注文本数据上进行训练，学习到语言的基本知识和潜在规律。预训练后的模型就是所谓的基座。

### 3.2、SFT微调（Supervised Fine-Tuning）

​		SFT微调是指对预先训练好的大型语言模型（如GPT系列）进行监督式微调。通过使用大量的人工标注数据，根据特定任务需求，进一步优化模型的性能。这些数据通常包括输入与对应的期望输出，让模型学会如何从输入得出正确的输出。微调的过程可以看作是在原始预训练模型的基础上，为其适应特定任务场景而进行的“个性化训练”。

### 3.3、奖励函数训练（Reward Modeling, RM）

​		RM训练是指为强化学习任务设计奖励函数。奖励函数是一个用于评估AI智能体在特定任务中表现的度量，引导智能体在学习过程中采取正确的行动。RBRM是一种基于排序的奖励建模方法，通过对多个候选输出进行人工排序，为输出赋予相对优劣，从而指导模型生成更好的回答。这种方法可以帮助解决常规奖励建模方法在一些情况下难以为模型提供足够明确指导的问题。

### 3.4、基于人类反馈的强化学习（RLHF）

​		PPO（Proximal Policy Optimization），即近端策略优化方法，是一种强化学习算法，通过优化模型的策略（即在给定输入时选择动作的方式）来提高模型性能。在基于RM或RBRM的PPO训练中，模型利用设计好的奖励函数（或基于排序的奖励模型）来学习如何为特定任务生成更好的输出。通过与环境交互并获取奖励信号，模型不断调整自身策略，以便在未来的相似任务中获得更高的奖励。PPO算法的优势在于其能够在保持稳定性的同时实现较高的性能。

### 3.5、与人类对齐（Align AI with human values）

​		大语言模型的与人类对齐是指让人工智能模型理解、遵循并适应人类的价值观、需求和期望。这意味着让模型在处理各种任务时，不仅要提供准确和有帮助的信息，还要确保所生成的内容遵循道德、法律和社会规范，避免产生有害或误导性的结果。

与人类对齐的过程通常包括以下几个方面：

1. 理解用户需求：让模型更好地理解用户在不同场景下的真实需求，以便生成有针对性和相关性的回答。
2. 遵循道德和法律规范：确保模型生成的内容符合道德和法律规定，避免侵犯隐私、传播虚假信息或煽动仇恨等行为。
3. 适应社会和文化背景：让模型了解并尊重不同社会和文化背景，以避免产生冒犯、歧视或误解的内容。
4. 防止有害输出：在模型生成内容时警惕并避免潜在的有害信息，确保输出无害且有益。

## 四、**基座模型**

### 4.1 **国外**模型

| 序号 | 名称              | 组织                           | 参数量                                                | 是否开源     | base模型                        | 特点                                                         | 推出日期         | 参考链接                                                     |
| ---- | ----------------- | ------------------------------ | ----------------------------------------------------- | ------------ | ------------------------------- | ------------------------------------------------------------ | ---------------- | ------------------------------------------------------------ |
| 1    | chatGPT           | openAI                         | 175B                                                  | 否           | GPT-3、                         |                                                              | 2022-12-24       | https://zhuanlan.zhihu.com/p/473001104                       |
| 2    | LLaMA             | mate                           | 70 亿、130 亿、330 亿和 650 亿这 4 种参数规模的模型。 |              |                                 | 参数仅为十分之一的 LLaMA-13 B 的性能优于 OpenAI 推出的 GPT3 (175 B)，也即支持 ChatGPT 的 GPT3.5 的前身。LLaMA-65 B 也可与业内领先的 Chinchilla-70 B 和 PaLM-540 B 竞争。 | 2023-2-27        |                                                              |
| 3    | OpenChatKit       | 原openAI团队                   | 20B                                                   | GPT-NeoX-20B | GPT-NeoX-20B                    |                                                              | 2023-3-13        | https://www.zhihu.com/question/589094770?s_r=0&utm_campaign=shareopn&utm_campaign=Sharon&utm_content=group3_supplementQuestions&utm_content=group3_supplementQuestions&utm_medium=social&utm_oi=32995419357184&utm_psn=1618884326426456064&utm_source=wechat_session |
| 4    | LaMDA             |                                |                                                       |              |                                 |                                                              |                  |                                                              |
| 5    | GPT-4             |                                |                                                       |              |                                 |                                                              |                  |                                                              |
| 6    | PaLM-rlhf-pytorch |                                |                                                       | 是           | Pathways系统                    | 其号称首个开源ChatGPT平替项目，其基本思路是基于谷歌语言大模型PaLM架构，以及使用从人类反馈中强化学习的方法（RLHF）。PaLM是谷歌在今年4月发布的5400亿参数全能大模型，基于Pathways系统训练。其可以完成写代码、聊天、语言理解等任务，并且在大多数任务上具有强大的少样本学习性能。同时采用了ChatGPT一样的强化学习机制，能让AI的回答更加符合情景要求，降低模型毒性。 | 2023-4           | https://github.com/lucidrains/PaLM-rlhf-pytorch              |
| 7    | GPTrillion        |                                |                                                       | 是           |                                 | 该项目号称开源的最大规模模型，高达1.5万亿，且是多模态的模型。其能力域包括自然语言理解、机器翻译、智能问答、情感分析和图文匹配等。 |                  | https://huggingface.co/banana-dev/GPTrillion                 |
| 8    | OpenFlamingo      | 非盈利机构LAION                |                                                       | 是           | 基于LLaMA的 OpenFlamingo-9B模型 | 目前开源的是其基于LLaMA的 OpenFlamingo-9B模型。Flamingo模型在包含交错文本和图像的大规模网络语料库上进行训练，具备上下文少样本学习能力。OpenFlamingo实现了原始Flamingo中提出的相同架构，在一个新的多模态C4数据集的5M样本和LAION-2B的10M样本上训练而来。 |                  | https://github.com/mlfoundations/open_flamingo               |
| 9    | llava             |                                |                                                       | 是           |                                 | LLaVA是一个多模态的语言和视觉对话模型，类似GPT-4，其主要还是在多模态数据指令工程上做了大量工作，目前开源了其13B的模型文件。从性能上，据了解视觉聊天相对得分达到了GPT-4的85%；多模态推理任务的科学问答达到了SoTA的92.53%。 | 2023年4月19日    |                                                              |
| 10   | miniGPT-4         | 沙特阿拉伯阿卜杜拉国王科技大学 |                                                       |              |                                 | 该项目对标GPT-4的能力域，实现了一个缩略版。该项目来自来自沙特阿拉伯阿卜杜拉国王科技大学的研究团队。该模型利用两阶段的训练方法，先在大量对齐的图像-文本对上训练以获得视觉语言知识，然后用一个较小但高质量的图像-文本数据集和一个设计好的对话模板对预训练的模型进行微调，以提高模型生成的可靠性和可用性。该模型语言解码器使用Vicuna，视觉感知部分使用与BLIP-2相同的视觉编码器。 | 2023年4月21日    | https://github.com/Vision-CAIR/MiniGPT-4                     |
| 11   | RedPajama         |                                | 1.2万亿token的数据集                                  | 是           |                                 | 目前LLama的授权比较有限，只能用作科研，不允许做商用。为了解决商用完全开源问题，RedPajama项目应运而生，其旨在创建一个完全开源的LLaMA复制品，可用于商业应用，并为研究提供更透明的流程。完整的RedPajama包括了1.2万亿token的数据集，其下一步将着手开始进行大规模训练。 |                  | https://github.com/togethercomputer/RedPajama-Data           |
| 12   | stanford-alpaca   | 斯坦福                         |                                                       | 是           | LLaMA-7B                        | 基本原理是让OpenAI的text-davinci-003模型以self-instruct方式生成52K指令样本，以此来微调LLaMA。该项目已将训练数据、生成训练数据的代码和超参数开源，模型文件尚未开源，以一天多达到5.6K星的关注度。该项工作由于成本低廉、数据易得，大受欢迎，也开启了低成本ChatGPT的效仿之路。 |                  | https://github.com/tatsu-lab/stanford_alpaca                 |
| 13   | **mPLUG-Owl**     |                                |                                                       |              |                                 | 与miniGPT-4、LLaVA类似，其是一个对标GPT-4的开源多模态大模型，其延续了mPLUG系列的模块化训练思想。其目前开源了7B参数量的模型，同时第一次针对视觉相关的指令理解提出一个全⾯的测试集 OwlEval，通过人工评测对比了已有模型，包括LLaVA、MiniGPT-4等工作，其展示出更优的多模态能力，尤其在多模态指令理解能力、多轮对话能力、知识推理能力等方⾯表现突出。 | **2023年5月7日** | https://github.com/X-PLUG/mPLUG-Owl                          |

### 4.2 **国内模型**

| 序号 | 名称                 | 组织                | 参数量      | 是否开源   | base模型                                                     | 特点                                                         | 推出日期      | 参考链接                                                     |
| ---- | -------------------- | ------------------- | ----------- | ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------- | ------------------------------------------------------------ |
| 1    | ChatYuan             | 元语智能            | 770M        | 是         | PromptCLUE-large（大规模多任务Prompt[预训练](https://so.csdn.net/so/search?q=预训练&spm=1001.2101.3001.7020)中文开源模型）base：t5-large（模型3G、训练要20G）推理：显存6G左右 | 披露的技术细节看，底层采用7亿参数规模的T5模型，并基于PromptClue进行了监督微调形成了ChatYuan。该模型基本上是ChatGPT技术路线的三步的第一步，没有实现奖励模型训练和PPO强化学习训练。它具有以下特点：• 大规模预训练：在t5-large版基础上，使用数百G中文语料，训练了100万步，累计训练了1.5万亿个中文字词级别token• 大规模任务数据：使用了16种任务类型，数百种任务，累积亿级别任务数据。• 混合预训练：一方面将下游任务作为预训练语料，另一方面将下游任务和预训练语料一起训练，减少任务灾难遗忘以及缩短预训练和下游任务的距离，更好的适应下游任务（ExT5）• 混合采样：针对众多数据量差异极大的任务，采用在每个训练batch内对所有的任务进行按照比例采样，根据任务的数据量进行平滑采样，并且同时限制任务数据量采样池的上限。平滑采样可以减少任务训练有偏危害，在每一batch内训练可以减少异质任务之间训练负迁移的情况(T5)• 分阶段训练：一方面指在预训练分阶段，涉及训练序列长度的分阶段（128和512），加快预训练速度(Bert)；另一方面，在下游训练分阶段， 涉及学习率和序列长度的变化以及递减式对下游任务的数据量限制，更好的适应下游的不同任务• 增加语言模型的训练：参考t5.1.1, 除了使用Span Corrpution构建的方式进行无监督训练，同时在使用prefix LM的方式训练，增强生成任务的能力(LM adapted)• 增加对模型的encoder以及decoder的训练：根据下游任务数据分别构建Data_text,Data_target预训练数据语料，是加入到预训练中，分别增强模型的encoder理解能力和 decoder的生成能力（见UIE）• 重新构建模型中文字典：使用sentencepiece上在千亿token上学习并构建模型字典，更加符合中文语言习惯 | 2023-2-3      | https://baijiahao.baidu.com/s?id=1756794078809809141&wfr=spider&for=pchttps://github.com/clue-ai/ChatYuan |
| 2    | BELLE                | 贝壳团队            |             | 是         | 中文版本的LLaMA                                              | • 开源了一个规模巨大的中文IFT数据集，现在加起来有300万以上，基本都是通过Self-Instructi得到• 做了一系列的实验，截止日前已发了4篇技术报告￮ 《Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences》￮ 《Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases》￮ 《Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation》￮ 《A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model》 |               | https://github.com/LianjiaTech/BELLE                         |
| 3、  | GLM-130              | 清华                | 130B        | 否，内侧中 |                                                              | • 双语：同时支持中文和英文；• 高精度（英文）：在公开的英文自然语言榜单 LAMBADA、MMLU 和 Big-bench-lite 上优于 GPT-3 175B（API: davinci，基座模型）、OPT-175B 和 BLOOM-176B；• 高精度（中文）：在 7 个零样本 CLUE 数据集和 5 个零样本 FewCLUE 数据集上明显优于 ERNIE TITAN 3.0 260B 和 YUAN 1.0-245B；• 快速推理：首个实现 INT4 量化的千亿模型，支持用一台 4 卡 3090 或 8 卡 2080Ti 服务器进行快速且基本无损推理；• 可复现性：所有结果（超过 30 个任务）均可通过我们的开源代码和模型参数复现；• 跨平台：支持在国产的海光 DCU、华为昇腾 910 和申威处理器及美国的英伟达芯片上进行训练与推理。 如今， 参考 ChatGPT 的设计思路，ChatGLM 在千亿基座模型 GLM-130B 中注入了代码预训练，通过有监督微调（Supervised Fine-Tuning）等技术实现人类意图对齐。 | 2023-3-15     | https://baijiahao.baidu.com/s?id=1760418691006236860&wfr=spider&for=pc |
| 4、  | ChatGLM              | 清华                | 6B          | 是         | 6B（ 62 亿）它是不同于 BERT、GPT-3 以及 T5 的架构，是一个包含多目标函数的自回归预训练模型。 | ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于GLM架构，具有62亿参数。ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约1T个标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持。 从技术路线上看，其实现了ChatGPT强化学习人类对齐策略，使得生成效果更佳贴近人类价值，其目前能力域主要包括自我认知、提纲写作、文案写作、邮件写作助手、信息抽取、角色扮演、评论比较、旅游建议等，目前其已经开发了正在内测的1300亿的超大模型，算是目前开源平替里面参数规模较大的对话大模型。它具有以下亮点：• GLM 是一种 Prefix decoder 的结构，而目前大模型采用的主流结构是 Casual decoder，可以说GLM走出了自己的道路，但究竟那种结构更优，需要更科学的评定• 1T 个token的中英文预训练，对比 175B 的 GPT3 训练了300B个 token，540B的 PaLM 训练了780B个 token，而 ChatGLM-6B 的底座只是6B的模型，却训练了1T个 token，让人有种憧憬，用大数据训练小模型，是否能达到小数据训练大模型的效果• 项目号称经过监督微调、反馈自助、人类反馈强化学习 | 2023-3-15     | github.com/THUDM/ChatGLM-6Bhttps://baijiahao.baidu.com/s?id=1760779859324903890&wfr=spider&for=pchttps://link.zhihu.com/?target=https%3A//chatglm.cn/blog |
| 5    | 文心一言             | 百度                |             | 否         |                                                              | 集成了 文心一阁、文心code等模型                              | 2023-3-16     |                                                              |
| 7    | 文心一言开源版本     |                     |             |            |                                                              | ![img](file:///C:\Users\Administrator\AppData\Local\Temp\ksohtml16052\wps3.png) |               | https://github.com/visual-openllm/visual-openllm             |
| 8    | colossalAI           |                     |             |            |                                                              | **监督数据集收集 -> 监督微调 -> 奖励模型训练 -> 强化学习微调的完整 RLHF 流程。详细步骤如下所示：**    第一阶段（stage1_sft.py）：SFT监督微调阶段，该开源项目没有实现，这个比较简单，因为ColossalAI无缝支持Huggingface，本人直接用Huggingface的Trainer函数几行代码轻松实现，在这里我用了一个gpt2模型，从其实现上看，其支持GPT2、OPT和BLOOM模型；    第二阶段（stage2_rm.py）：奖励模型（RM）训练阶段，即项目Examples里train_reward_model.py部分；    第三阶段（stage3_ppo.py）：强化学习（RLHF）阶段，即项目train_prompts.py    三个文件的执行需要放在ColossalAI项目中，其中代码中的cores即原始工程中的chatgpt，cores.nn在原始工程中变成了chatgpt.models |               | https://mp.weixin.qq.com/s/V5pCvYvkPXwiMw-FNIErXwhttps://github.com/hpcaitech/ColossalAI |
| 9    | ChatDoctor           |                     |             |            |                                                              |                                                              |               | https://github.com/Kent0n-Li/ChatDoctor                      |
| 10   | LLaMA                |                     |             |            | LLaMA                                                        | https://scontent-frt3-2.xx.fbcdn.net/v/t39.8562-6/333078981_693988129081760_4712707815225756708_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=0aDjyi_Cgw8AX_Kc_V2&_nc_ht=scontent-frt3-2.xx&oh=00_AfDrvZe32vjVnLFkFs0LJXvme2ZLmy7kRheQbwQ6TeGq-Q&oe=64313FE2 |               |                                                              |
| 11   | **ColossalChat**     |                     |             |            | LLaMA                                                        | https://zhuanlan.zhihu.com/p/613315873                       |               | https://mp.weixin.qq.com/s/V5pCvYvkPXwiMw-FNIErXwhttps://github.com/hpcaitech/ColossalAI |
| 12   | Alpaca               |                     |             | 是         | LLaMA                                                        | https://arxiv.org/pdf/2212.10560.pdf                         |               | https://github.com/tatsu-lab/stanford_alpacahttps://github.com/tloen/alpaca-lora |
| 13   | **Luotuo**           |                     |             |            | LLaMA                                                        | 是第一个基于LLaMA预训练模型开源完整RLHF pipline实现，包括有监督数据收集、有监督微调、奖励模型训练和强化学习微调。只需要不到100亿个参数，就可以在大型语言模型的基础上通过RLHF微调达到中英文双语水平，达到与ChatGPT和GPT-3.5相当的效果，并可以进行Demo测试。关于RLHF的原理，可参考https://zhuanlan.zhihu.com/p/613315873 |               | https://github.com/LC1332/Luotuo-Chinese-LLM                 |
| 14   | LMFlow               |                     |             |            | LLaMA                                                        | https://view.inews.qq.com/a/20230402A02S2F00                 |               | https://github.com/OptimalScale/LMFlow                       |
| 15   | Chinese-LLaMA-Alpaca | 科大讯飞&哈工大团队 |             | 是         | LLaMA                                                        | 该项目包括词表扩充、继续预训练和指令精调三部分，其中词表扩充的代码参见 [merge_tokenizers.py](https://link.zhihu.com/?target=https%3A//github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/scripts/merge_tokenizers.py) ；预训练和指令精调代码参考了 transformers中的 [run_clm.py](https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py) 和 [Stanford Alpaca](https://link.zhihu.com/?target=https%3A//github.com/tatsu-lab/stanford_alpaca) 项目中数据集处理的相关部分。• 相对完整的流程，不紧紧是指令微调这一步，还包括词表扩充、继续预训练• 针对LLaMA模型扩充了中文词表，提升了中文编解码效率。这一步是我觉得假如想用LLaMA去做中文必须要做的一个事情，因为原生LLaMA对中文支持不好，很多中文词不在它们的词表中，会被切成两个token，极大影响了效果• 对中文继续做了20G语料的预训练，这个预料规模一看就很熟，它们开源的RoBERTa、MacBERT也是在这个规模的预料上训练得到的• 在预训练介绍，分成两部分，第一阶段：冻结transformer参数，仅训练embedding，在尽量不干扰原模型的情况下适配新增的中文词向量，第二阶段：使用LoRA技术，为模型添加LoRA权重（adapter），训练embedding的同时也更新LoRA参数。这给大家做高效继续预训练的提供了一个方向。 |               | https://github.com/ymcui/Chinese-LLaMA-Alpaca                |
| 16   | ChatLLaMA            |                     |             |            | LLaMA                                                        |                                                              |               | https://github.com/ydli-ai/Chinese-ChatLLaMA                 |
| 17   | **DeepSpeed Chat**   | 微软                |             |            | **OPT-13B**                                                  | https://mp.weixin.qq.com/s/ywIyWG3p3fhsCR688k7ipA            |               | https://github.com/microsoft/DeepSpeed                       |
| 18   | Dolly 2.0            | Databricks          |             |            |                                                              | https://mp.weixin.qq.com/s/KTtcqekgn58XH-mUobBCvQ            |               | https://huggingface.co/databricks/dolly-v2-12                |
| 19   | Open-Llama           |                     |             |            |                                                              | Open-Llama是一个开源项目，提供了一整套用于构建大型语言模型的训练流程及代码，从数据集准备到分词、预训练、指令调优，以及强化学习技术 RLHF |               | https://github.com/chenfeng357/open-Chinese-ChatLLaMA        |
| 20   | Moss                 | 复旦大学            | 参数量大16B | 是         |                                                              | 今年2月21日，复旦大学发布了MOSS，并开放公测，在公测崩溃后引起一些争议。现在该项目迎来重要更新和开源。开源的MOSS支持中英两个语种，且支持插件化，如解方程、搜索等。参数量大16B，在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。 | 2023年4月21日 | https://github.com/OpenLMLab/MOSS                            |
| 21   | **Alpaca-CoT**       | 中科院（学生）      |             | 是         | LLaMA、ChatGLM、BLOOM                                        | • 统一了多个底座模型，包括有 LLaMA、ChatGLM、BLOOM• 整理统一了其他公开项目的数据集，如果大家想梳理下现在市面上开源的IFT数据集，建议可以通过这个项目了解• 项目集成了 Int8-bitsandbytes、Fp16-mixed precision、LoRA（hugging peft库）等高效训练的方法• 首个加入了 CoT 训练后的效果研究 |               | https://github.com/PhoebusSi/Alpaca-CoT/blob/main/CN_README.md |
| 22   | StackLLaMA           | Hugging Face        |             | 是         |                                                              | Hugging Face的研究人员发布的一个70亿参数的模型——StackLLaMA。这是一个通过人类反馈强化学习在LLaMA-7B微调而来的模型。• 博客更像是一个教程和指南，介绍如何使用RLHF来训练模型，而不是主要关注模型的性能表现• 重点介绍了指令微调和强化学习部分，其中强化学习部分利用的是StackOverflow构建的数据集，利用网民们的upvotes去进行评分，给我们构造强化学习数据集部分提供了一个思路，能否利用微博点赞、知乎点赞等数据去构造一个中文的强化学习数据集呢？ |               | https://huggingface.co/blog/stackllama https://link.zhihu.com/?target=https%3A//huggingface.co/spaces/trl-lib/stack-llama |
| 23   | **华驼(HuaTuo)**     |                     |             | 是         |                                                              | 本项目开源了经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。我们通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果。 |               | https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese          |

 



## **7、参考文献**

（1）https://zhuanlan.zhihu.com/p/562736918

（2）[汇总那些ChatGPT的平替们，仅汇总那些开源代码](https://github.com/chenking2020/FindTheChatGPTer)

（3）https://zhuanlan.zhihu.com/p/624079704?utm_source=wechat_session&utm_medium=social&s_r=0

 (4)    https://zhuanlan.zhihu.com/p/609716668

（5）https://github.com/Hannibal046/Awesome-LLM

（6）https://github.com/mli/paper-reading

（7）[http://rank.chinaz.comm.datalearner.com](http://rank.chinaz.comm.datalearner.com/ai-models/pretrained-models?aiArea=-1&openSource=是&publisher=-1)

 
