# 垂直领域大模型构建和微调

* ## 一、**大模型训练方式**

  ### **1.1、知识库引入**

  Fine-tuning：Fine-tuning是指在已经训练好的GPT/LLM模型基础上，使用新的数据集再次进行训练。这种方法可以使模型针对特定任务或特定领域的语言使用情况进行优化，从而提高模型的效果。在Fine-tuning过程中，可以将额外的知识作为新的数据集加入到训练中。

  Knowledge Distillation：Knowledge Distillation是指将一个“大模型”的知识转移到一个“小模型”中。例如，可以将一个已经训练好的GPT/LLM模型的知识转移到一个小型的模型中，使得小型模型能够使用大型模型中的知识。这种方法可以通过将额外的知识加入到“大模型”中，从而使得“小模型”可以使用这些知识。

  数据增强：数据增强是指在已有的数据集中，添加一些类似但不完全相同的数据来增加数据的数量和多样性。这种方法可以使得模型更加全面地学习到不同的语言使用情况，从而提高模型的效果。在数据增强的过程中，可以添加额外的知识，例如同义词、反义词、专业术语等。

  多任务学习：多任务学习是指同时训练一个模型完成多个任务。例如，在训练GPT/LLM模型时，可以让模型同时完成文本分类、情感分析等任务，从而使得模型可以学习到更加多样化的知识。在多任务学习的过程中，可以将额外的知识添加到其他任务中，从而间接地影响到模型在主要任务上的表现。

  ### 1.2、**任务训练**

  | 序号 | 主题                | 方案                                                         | 参考资料                                                     |
  | ---- | ------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | 1    | ChatGLM-6B 训练方式 | ChatGLM-6B的下游任务的各种微调                               | https://github.com/liucongg/ChatGLM-Finetuning               |
  | 1    | 训练方式            | 综述                                                         | https://mp.weixin.qq.com/s/roP16HkHLMd9g1pwFL0dFQ**大模型LLM-微调经验分享&总结**https://zhuanlan.zhihu.com/p/620885226?utm_campaign=shareopn&utm_medium=social&utm_oi=965278498546425856&utm_psn=1630298786307178497&utm_source=wechat_session&s_r=0 |
  | 2    | 训练方式            | LoRA: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS     | https://arxiv.org/pdf/2106.09685.pdfLLM+LoRa微调加速技术原理及基于PEFT的动手实践：一些思考和mt0-large+lora完整案例https://mp.weixin.qq.com/s/xpbJ6qjLpp1IO5WEvTwKMQ |
  | 3    | 训练方式            | Prefix Tuning: P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks | 论文：https://arxiv.org/pdf/2110.07602.pdf                   |
  | 4    | 训练方式            | Prompt-Tuning：The Power of Scale for Parameter-Efficient Prompt Tuning | Prompt-Tuning——深度解读一种新的微调范式：https://wjn1996.blog.csdn.net/article/details/120607050?spm=1001.2014.3001.5502论文：https://arxiv.org/pdf/2104.08691.pdf |
  | 5    | 训练方式            | P-Tuning: GPT Understands, Too                               | 论文：https://arxiv.org/pdf/2103.10385.pdf                   |
  | 6    | 训练方式-工程化     | PEFT库                                                       | https://github.com/huggingface/peft                          |
  | 7    | 基于先验知识训练    | 如何为GPT/LLM模型添加额外知识？                              | https://www.zhihu.com/question/591935281/answer/2956166127?utm_campaign=shareopn&utm_content=group3_Answer&utm_medium=social&utm_oi=645306767316226048&utm_psn=1624756838607290368&utm_source=wechat_session |
  | 8    | 训练参数            |                                                              | **temperature**：控制生成文本的随机性。较高的温度会导致更加随机和多样化的生成文本，而较低的温度则会更加保守和精准。取值范围为0到1，一般默认为0.5。<br />**top_p**：指定生成文本的多样性。该参数与温度类似，可以控制生成文本的随机性，但是会更加保守和精准。如果设置了top_p，则在保证生成文本的概率总和超过top_p之前，会一直选择概率最高的单词进行生成。一般取值范围为0到1，一般默认为1.0。<br />**frequency_penalty**：控制重复单词的惩罚力度。较大的惩罚力度会导致生成文本中不太可能出现相同的单词，而较小的惩罚力度则会容忍一定程度的重复。取值范围为0到1，一般默认为0。<br />**presence_penalty**：控制模型生成与文本样本中不同的单词的惩罚力度。较大的惩罚力度会导致生成文本中更多地包含文本样本中未出现的单词，而较小的惩罚力度则会限制生成文本的多样性。取值范围为0到1，一般默认为0。<br />**best_of**：指定API返回多少个完整的响应中的最佳响应。如果指定了best_of，则API将生成多个响应并从中选择最佳响应。<br />**timeout**：指定等待API返回响应的最长时间（以毫秒为单位）。<br />**echo**：指定是否返回输入的请求。<br />**prompt**：指定API的输入文本。<br />**engine**：指定要使用的OpenAI语言模型。<br />**max_tokens**：指定API生成文本的最大长度。<br />**n**：指定API生成的文本数量。<br />**stop**：指定当API生成的文本中出现此字符串时停止生成文本 |
  | 9    | 训练参数            | 训练参数关于几个参数的含义，topk，top  p，temperature        | Transformers仓库做语言生成的解码方法介绍https://zhuanlan.zhihu.com/p/430961578?utm_id=0https://huggingface.co/blog/how-to-generatehttps://arxiv.org/pdf/1908.04319.pdf如何让大模型生成解码阶段的结果更好：从Beam Search到top_k、top_p等参数的实现原理与脚本实现https://mp.weixin.qq.com/s/IswrgDEn94vy5dCO51I1sw |