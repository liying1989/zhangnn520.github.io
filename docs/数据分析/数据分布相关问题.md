---
sort: 1
---

# 数据分布相关问题

> 持续更新中


[🔨算法开发手册](https://kg-nlp.github.io/Algorithm-Project-Manual/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98.html)


[🔨个人知乎](https://www.zhihu.com/people/zhangyj-n)




* [迁移学习中为什么要研究边缘概率分布和条件概率分布](https://www.zhihu.com/question/293820673)
    * 迁移学习的一大问题是领域自适应问题，即希望在有大量标注数据训练的源域中训练的模型，在只有少量标注或者无标注数据的目标域中有很好的泛化性。其中的挑战在于源域和目标域的数据分布很可能是不同的，即源域和目标域的联合概率分布可能是不同的。  
    * 由贝叶斯公式可将联合概率计算分为边缘概率分布和条件概率分布。
    * 条件概率分布相同，也就是学习任务相同，但源域和目标域的输入边缘分布可能不同，或是标签分布可能不同。输入边缘分布不同，我们设计一个统一的标签，使得映射一致；标签分布不同，我们使数据分布尽量均衡


* [机器学习数据不满足同分布，怎么整？](https://github.com/aialgorithm/Blog/issues/63)
    * 什么是数据不满足同分布
    * 为什么数据不满足同分布
    * 如何检测数据满足同分布
    * 如何解决数据不满足同分布

* [Adversarial Validation解决分布不一致问题](https://www.kaggle.com/code/kevinbonnes/adversarial-validation/notebook)
    *  核心思路是构建一个分类模型，目的是分辨训练集和测试集的来源，这里假设使用AUC作为分类精度评价函数。
    *  如果分类模型无法分辨样本（AUC接近0.5），则说明训练集和测试集数据分布比较一致；
    *  如果分类模型可以很好分辨样本（AUC接近1），则说明训练集和测试集数据分布不太一致；
    *  可以通过字面匹配或是语义检索类方法从未标注的数据中筛选和测试集相似的数据(在分类任务中实践)[场景实践(一)](https://kg-nlp.github.io/Algorithm-Project-Manual/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/%E5%9C%BA%E6%99%AF%E5%AE%9E%E8%B7%B5(%E4%B8%80).html)
    *  其他参考资料[paper](https://paperswithcode.com/paper/adversarial-validation-approach-to-concept)  [Kaggle知识点](https://zhuanlan.zhihu.com/p/136057721)


## 专业概念

* 协变量    
    *  协变量是不在受控制（研究范围）的自变量内，但仍会影响研究结果的变量。
    *  研究某种自变量对因变量的影响，则实验过程中除研究的自变量因变量之外，还有其他很多变量对实验造成影响，而这些其他变量中，可以被控制的叫控制变量，不可被控制的叫协变量
* 联合概率
    *  包含多个条件且所有条件同时成立的概率，记作P(X=a,Y=b)或P(a,b)，有的书上也习惯记作P(ab)
* 边缘概率
    *  边缘概率是与联合概率对应的，P(X=a)或P(Y=b)，这类仅与单个随机变量有关的概率称为边缘概率
* 条件概率
    *  条件概率表示在条件Y=b成立的情况下，X=a的概率，记作P(X=a\|Y=b)或P(a\|b)
* 贝叶斯公式
    *  先验概率：知道原因推结果的，P(原因)、P(结果\|原因)等
    *  后验概率：根据结果推原因的，P(原因\|结果)等
    *  贝叶斯公式解决的是一些原因X无法直接观测、测量，而我们希望通过其结果Y来反推出原因X的问题，也就是知道一部分先验概率，来求后验概率的问题。
* element-wise
    *  快速逐元素element-wise product = element-wise multiplication = Hadamard product 两个矩阵对应位置元素进行乘积
* batch normalization
    * BN 独立地规范化每一个输入维度`$x_i$` ，但规范化的参数是一个 mini-batch 的一阶统计量和二阶统计量。这就要求 每一个 mini-batch 的统计量是整体统计量的近似估计，或者说每一个mini-batch彼此之间，以及和整体数据，都应该是近似同分布的。分布差距较小的mini-batch可以看做是为规范化操作和模型训练引入了噪声，可以增加模型的鲁棒性；但如果每个mini-batch的原始分布差别很大，那么不同 mini-batch 的数据将会进行不一样的数据变换，这就增加了模型训练的难度。 
    因此，BN 比较适用的场景是：每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多。
* Layer normalization
    *  LN 针对单个训练样本进行，不依赖于其他数据,以用于 小mini-batch场景



## 直观理解

* **==分类任务中标注数据覆盖不足以及数据偏置问题==** [参考链接](https://zhuanlan.zhihu.com/p/636063567)

对于数据覆盖不足的问题,可以通过查看标注数据中训练集和测试集关键词的占比情况来判断标注数据的合理性，如果对于某个类别训练集下的关键词占测试集超过80%,我们就认为标注数据覆盖程度足够,可以使用当前测试集验证,否则需要补充训练集

以下是分析数据覆盖情况的代码
```python
import os
import pandas as pd
from collections import defaultdict,Counter
import json
from tqdm import tqdm
import random
from copy import deepcopy
# from LAC import LAC
from paddlenlp import Taskflow
import matplotlib.pyplot as plt

def print_log(text:str):
    interval = int((100-len(text)*2)/2)
    print('-' * interval,text,'-' * interval)

def keyword_label_distribution():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--output_dir",
                        default=text_distribution_dir,  # 自定义
                        type=str,
                        help="文本标签统计输出路径")
    parser.add_argument("--train_path", type=str, default=train_path, help="训练集")
    parser.add_argument("--dev_path", type=str, default=dev_path, help="测试集")
    parser.add_argument("--label_path", type=str, default=label_path, help="训练集标签")
    parser.add_argument('--cnt_threshold', type=int, default=3, help='至少n句包含某个关键词')
    parser.add_argument('--p_threshold', type=float, default=0.5,
                        help='输出最低偏置度阈值')
    parser.add_argument('--cover_degree', type=bool, default=True,
                        help='训练集关键词覆盖度')
    parser.add_argument('--which_data', type=str, default='train',choices=['train','dev','all'],  # 先设置为train,再设置为dev
                        help='统计数据集类别')
    args = parser.parse_args()
    class BiasWord(object):
        """
        Statistic the biased words in the dataset
        """
        def __init__(self, segments:list, labels:list, num_classes=24, cnt_threshold=3, p_threshold=0.85):
            self.cnt_threshold = cnt_threshold
            self.p_threshold = p_threshold
            self.num_classes = num_classes
            self.segments = segments  # [[]]
            self.labels = labels # []

        def process(self):
            self._get_dict()
            self._search_bias_word()
            print("number of bias_words:", len(self.word2label))
            return self.word2label, self.bias_word_cnt, self.label2word
        def _get_dict(self):
            self.word2sentids = defaultdict(set)  # 单词:(句子索引)
            self.label2word = defaultdict(dict)  # 标签:{单词:频数(一个句子只取一次)}
            self.sentid2words = defaultdict(set)  # 句子索引:(单词)
            label2word = defaultdict(list)
            for n, segs in enumerate(self.segments):
                for seg in segs:
                    self.word2sentids[seg].add(n)  # 每个词对应的句子索引,句子的索引是set(不考虑一句出现多个相同的词)
                self.sentid2words[n] = set(segs)  # 每个句子索引对应的句子分词,句子分词列表set
                label2word[id2label[self.labels[n]]].extend(list(set(segs)))
            # 统计标签对应的关键词
            for k,v in label2word.items():
                self.label2word[k] = dict(sorted(dict(Counter(v)).items(),key=lambda x:x[1],reverse=True))
        def _search_bias_word(self):
            self.word2label = {}
            self.bias_word_cnt = {}
            for word, sentids in self.word2sentids.items():
                if len(sentids) >= self.cnt_threshold:  # 单词出现的句子数量
                    cnts = [0] * self.num_classes
                    for sentid in sentids:
                        label_id = self.labels[sentid]  # 句子索引对应的标签id
                        cnts[label_id] += 1
                    assert sum(cnts) != 0
                    max_cnt = max(cnts)  # 统计出现次数最多的标签
                    p = max_cnt / sum(cnts)  # 统计出现次数最多的标签占比
                    if p >= self.p_threshold:
                        #     self.word2label[word] = p  # 偏置词存在的最大偏置度(没有指定哪个标签)
                        #     self.bias_word_cnt[word] = len(sentids)  # 包含偏置词的句子数量
                        self.bias_word_cnt[word] = len(sentids)  # 包含偏置词的句子数量
                        # 此处修改增加每个关键词对应的标签和标签分布
                        # 统计关键词对应的标签以及占比
                        total_cnts = sum(cnts)
                        label_p_statistic = {}
                        for label_id,num in enumerate(cnts):
                            if num == 0: continue
                            label_p_statistic[id2label[label_id]] = round(num/total_cnts,2)
                        self.word2label[word] = dict(sorted(label_p_statistic.items(),key=lambda x:x[1],reverse=True))

    print_log('覆盖度计算')  # 下面是对生成的分词偏置文件进行处理,生成覆盖分布的文件
    if os.path.exists(os.path.join(args.output_dir, 'dev_label2word.json')) and \
                      os.path.exists(os.path.join(args.output_dir, 'train_label2word.json')) and args.cover_degree:
        print_log('计算覆盖度')
        fr_train_dict = json.load(open(os.path.join(args.output_dir, 'train_label2word.json'),'r'))
        fr_dev_dict = json.load(open(os.path.join(args.output_dir, 'dev_label2word.json'),'r'))
        check_dict = defaultdict(dict)
        for k,v in tqdm(fr_dev_dict.items(),desc='覆盖分布提取'):
            temp_set = set(list(fr_train_dict[k].keys()))
            num = 0
            sum_num = len(v)  # 测试集该标签下所有关键词
            for i in v.keys():
                if i in temp_set:  # 测试集下所有数据中 在训练集中的关键词,剩下的都不在该范围内
                    num += 1
            check_dict[k] =  {'测试标签对应关键词数量':sum_num,
                           '训练集未覆盖关键词数量':sum_num-num,
                           '未覆盖比例':(sum_num-num)/sum_num,
                            '覆盖比例':num/sum_num}
        fw_proportion = open(os.path.join(args.output_dir, 'word_label_proportion.json'),'w')
        json.dump(check_dict, fw_proportion, indent=2, ensure_ascii=False)
        fw_proportion.close()
        df = pd.DataFrame.from_dict(check_dict,orient='index')
        plt.figure(dpi=300, figsize=(10, 8))
        df.plot.barh(y=['未覆盖比例','覆盖比例'],stacked=True)
        plt.show()
        # print(df.head())

        return  # 执行完直接返回

    print_log('提取关键词')
    with open(stopwords_path, 'r', encoding='utf-8') as f:  #
        stopword_list = set([word.strip('\n') for word in f.readlines()])
    ner_fast = Taskflow("ner", mode='fast', batch_size=1, user_dict=userdict_path)
    pos_flag = set(['n', 'nz', 'vn', 'an', 'v'])
    # 获取label2id和id2label,以及数量num_classes
    label2id = {}
    id2label = {}
    for id, label in enumerate(open(args.label_path, 'r').readlines()):
        label2id[label.strip()] = id
        id2label[id] = label.strip()
    num_classes = len(label2id)
    segments = []
    labels = []
    lines = []

    if args.train_path and (args.which_data == 'train' or args.which_data == 'all'):
        with open(args.train_path, 'r') as f:  # 这部分根据自己的文件修改处理方法,目标是生成分词
            for line in tqdm(list(f)[1:],desc='训练集分词'):
                lines.append(line)
                task_type,task_name,prefix_name,label = line.strip().split('\t')  # 我这里的数据是四列
                if not task_name:task_name='空'
                if not prefix_name:prefix_name='空'
                seg_res = ner_fast([task_name, prefix_name])
                task_prefix_segs = []
                for i in seg_res[0]+seg_res[1]:
                    if i[0] not in stopword_list and len(i[0]) > 1 and i[1] in pos_flag:
                        task_prefix_segs.append(i[0])
                segments.append(task_prefix_segs)
                labels.append(label2id[label])
    if args.dev_path and (args.which_data == 'dev' or args.which_data == 'all'):
        with open(args.dev_path, 'r') as f:
            for line in tqdm(list(f)[1:],desc='验证集分词'):
                lines.append(line)
                task_type, task_name, prefix_name, label = line.strip().split('\t')
                if not task_name:task_name='空'
                if not prefix_name:prefix_name='空'
                seg_res = ner_fast([task_name, prefix_name])
                task_prefix_segs = []
                for i in seg_res[0] + seg_res[1]:
                    if i[0] not in stopword_list and len(i[0]) > 1 and i[1] in pos_flag:
                        task_prefix_segs.append(i[0])
                segments.append(task_prefix_segs)
                labels.append(label2id[label])

    # get bias_words
    print_log('获取偏置数据')
    biasword = BiasWord(segments, labels, num_classes=num_classes, cnt_threshold=args.cnt_threshold,
                        p_threshold=args.p_threshold)
    # b_words: biased words, dict
    # b_word_cnt: count of biased words, dict
    # sentids2words: sentence index to words, dict
    # b_words, b_word_cnt, sentids2words = biasword.process()
    print_log('写入文件')
    word2label, b_word_cnt, label2word = biasword.process()
    bias_word = 'word2label.json'
    bias_word_cnt = 'bias_word_cnt.json'
    bias_babel = 'label2word.json'
    if args.which_data=='all':
        pass
    elif args.which_data=='train':
        bias_word = 'train_word2label.json'
        bias_word_cnt = 'train_bias_word_cnt.json'
        bias_babel = 'train_label2word.json'
    elif args.which_data=='dev':
        bias_word = 'dev_word2label.json'
        bias_word_cnt = 'dev_bias_word_cnt.json'
        bias_babel = 'dev_label2word.json'

    # save result to output_dir
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
    with open(os.path.join(args.output_dir, bias_word), 'w') as f:
        json.dump(word2label, f, indent=2, ensure_ascii=False)
    with open(os.path.join(args.output_dir, bias_word_cnt), 'w') as f:
        json.dump(b_word_cnt, f, indent=2, ensure_ascii=False)
    with open(os.path.join(args.output_dir, bias_babel), 'w') as f:
        json.dump(label2word, f, indent=2, ensure_ascii=False)
    # with open(os.path.join(args.output_dir, "sentids2words2words.json"), 'w') as f:
    #     for k, v in sentids2words2words.items():
    #         sentids2words2words[k] = list(v)
    #     json.dump(sentids2words, f, indent=2, ensure_ascii=False)
```

* 文件示例

train_word2label.json

```
{
  "单词1": {
    "标签1": 0.62,
    "标签2": 0.31,
    "标签3": 0.08
  },
  "单词2":{...}
```

train_bias_word_cnt.json


```
{
  "单词1": 13,
  "单词2": 1340,
  ...
 }
```

train_label2word.json

```
{
  "标签1": {
    "单词1": 726,
    "单词2": 520,
    ...
    },
  "标签2":{...}
```

word_label_proportion

```
{
  "标签1": {
    "测试标签对应关键词数量": 215,
    "训练集未覆盖关键词数量": 42,
    "未覆盖比例": 0.19534883720930232,
    "覆盖比例": 0.8046511627906977
  },
  "标签2": {
    "测试标签对应关键词数量": 179,
    "训练集未覆盖关键词数量": 21,
    "未覆盖比例": 0.11731843575418995,
    "覆盖比例": 0.88268156424581
  }
  ...
 }
 这个结果说明训练集覆盖度还可以
```


## 参考链接

* [在迁移学习中，边缘概率分布和条件概率分布有什么含义？](https://www.zhihu.com/question/293820673)
* [详解深度学习中的Normalization，BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)
* [深度学习中 Batch Normalization为什么效果好？](https://www.zhihu.com/question/38102762/answer/85238569)
* [全连接层的作用是什么？](https://www.zhihu.com/question/41037974/answer/150522307)
* [numpy的数组和向量化计算](https://pydata.readthedocs.io/zh/latest/chapters/p04_numpy_basics_arrays_and_vectorized_computation.html)
* [BatchNorm和LayerNorm——通俗易懂的理解](https://blog.csdn.net/Little_White_9/article/details/123345062)

* [BatchNorm与LayerNorm的理解](https://zhuanlan.zhihu.com/p/608655896)
* [一文搞懂Batch Normalization,Layer/Instance/GroupNorm](https://zhuanlan.zhihu.com/p/152232203)
* [AI疯狂进阶——正则化篇播](https://baijiahao.baidu.com/s?id=1653085297096293714&wfr=spider&for=pc)
* [关于weight_decay的深度分析](https://zhuanlan.zhihu.com/p/339448370)
* [深度学习必须掌握的 13 种概率分布](https://mp.weixin.qq.com/s/gA581-ksmTtfJsXWG1ogPQ)