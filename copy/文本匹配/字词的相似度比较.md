---
sort: 3
---


# 字词相似度比较


* [算法开发手册](https://kg-nlp.github.io/Algorithm-Project-Manual/文本匹配/字词相似度.html)

* [个人知乎](https://www.zhihu.com/people/zhangyj-n)


---
* 项目冷启动时没有标注数据
* 没有类似开源roformer sim的模型可以计算相似度
* 没有LLM提供服务

## 无监督

* average word vectors：简单的对句子中的所有词向量取平均，是一种简单有效的方法，
缺点：没有考虑到单词的顺序，只对15个字以内的短句子比较有效，丢掉了词与词间的相关意思，无法更精细的表达句子与句子之间的关系。
* tfidf-weighting word vectors：指对句子中的所有词向量根据tfidf权重加权求和，是常用的一种计算sentence embedding的方法，在某些问题上表现很好，相比于简单的对所有词向量求平均，考虑到了tfidf权重，因此句子中更重要的词占得比重就更大。
缺点：没有考虑到单词的顺序
* bag of words：这种方法对于短文本效果很差，对于长文本效果一般，通常在科研中用来做baseline。缺点：1.没有考虑到单词的顺序，2.忽略了单词的语义信息。
* LDA：计算出一片文档或者句子的主题分布。也常常用于文本分类任务
* smooth inverse frequency（简称SIF)为权重，对所有词的word vector加权平均，最后从中减掉principal component，得到sentence embedding
[1] Sanjeev Arora, et al. 2017. A Simple but Tough-to-Beat Baseline for Sentence Embeddings
* Word Mover’s Distance[2]（简称WMD），直接度量句子之间的相似度
[2] Matt J. Kusner, et al. 2015. From Word Embeddings To Document Distances
* LSI或LSA：LSI是处理相似度的，基于SVD分解，用于特征降维，LSI求解出来的相似度跟topic相关性很强，而句子结构等信息较少。顺便说下，句子中词的顺序是不会影响LSI相似度结果的。
* TF-IDF
    * TF的定义
        * 有一个专门的术语来表示关键字出现的次数，叫“词频”(Term Frequency), 简写为TF。TF越大，通常相关性越高。但是，你可能会发现一个问题。 如果一篇小短文里出现了一次“Lucence”，而一部好几百页的书里提到两次“Lucence”，我们不会认为那部书与Lucence相关性更高。为了消除文档本身大小的影响，一般使用TF时会把文本长度考虑上：
        `TF Score ＝ 某个词在文档中出现的次数 ／ 文档的长度`
       举例：某文档D，长度为200，其中“Lucence”出现了2次，“的”出现了20次，“原理”出现了3次，那么:
        `TF(Lucence|D) = 2/200 = 0.01`  
        `TF(的|D) = 20/200 = 0.1`
        `TF(原理|D) = 3/200 = 0.015`
     “Lucence的原理”这个短语与文档D的相关性就是三个词的相关性之和。
      `TF(Lucence的原理|D) = 0.01 + 0.1 + 0.015 = 0.125`
        * 我们发现一个问题，就是“的”这个词占了很大权重，而它对文档主题的几乎没什么贡献。这种词叫停用词，在度量相关性时不考虑它们的词频。去掉这个词后，上面的相关性变为0.025。其中“Lucence”贡献了0.01, “原理”贡献了0.015。
        细心的人还会发现，“原理”是个很通用的词，而“Lucence”是个专业词。直觉告诉我们，“Lucence”这个词对我们的搜索比“原理”更重要。抽象一下，可以理解为 一个词预测主题的能力越强，就越重要，权重也应该越大。反之，权重越小。
        * 假设我们把世界上所有的文档的总和看成一个文档库。如果一个词，很少在文档库里出现过，那通过它就容易找到目标，它的权重也应该大。反之，如果一个词在文档库中大量出现，看到它仍然不清楚在讲什么内容，它的权重就应该小。“的、地、得”这些虚词出现的频率太高，以至于权重设为零也不影响搜素，这也是它们成为停用词的原因之一。
    * IDF的定义
        * 假设关键词w在n个文档中出现过，那么n越大，则w的权重越小。常用的方法叫“逆文本频率指数”(Inverse Dcument Frequency, 缩写为IDF)。一般的：`IDF = log(N/n)`
        注意: 这里的log是指以2为底的对数,不是以10为底的对数。
        N表示全部文档数。假如世界上文档总数位100亿，"Lucence"在1万个文档中出现过，“原理”在2亿个文档中出现过，那么它们的IDF值分别为：
        `IDF(Lucence) = log(100亿/1万) = 19.93`
        `IDF(原理) ＝ log(100亿/2亿) ＝ 5.64`
        * “Lucence”重要性相当于“原理”的3.5倍。停用词“的”在所有的文档里出现过，它的IDF=log(1)=0。短语与文档的最终相关性就是TF和IDF的加权求和：
        `simlarity = TF1*IDF1 + TF2*IDF2 + ... + TFn*IDFn`
        * 现在可以计算出上文中提到的“Lucence的原理”与文档D的相关性:
        `simlarity(Lucence的原理|D) = 0.01*19.93 + 0 + 5.64*0.015 ＝ 0.2839`
        其中，“Lucence”占了70%的权重，“原理”仅占30%的权重。
    
    * Lucence中的TF-IDF
        * 早期的Lucence是直接把TF-IDF作为默认相似度来用的，只不过做了适当调整，它的相似度公式为:
        `simlarity = log(numDocs / (docFreq + 1)) * sqrt(tf) * (1/sqrt(length))`
        *  numDocs:索引中文档数量，对应前文中的N。lucence不是(也不可能)把整个互联网的文档作为基数，而是把索引中的文档总数作为基数。
        * docFreq: 包含关键字的文档数量，对应前文中的n。
        * tf: 关键字在文档中出现的次数。
        * length: 文档的长度。
        * 上面的公式在Lucence系统里做计算时会被拆分成三个部分：
        `IDF Score = log(numDocs / (docFreq + 1))`
        `TF Score = sqrt(tf)`
        `fieldNorms = 1/sqrt(length)`
        * fieldNorms 是对文本长度的归一化(Normalization)。所以，上面公式也可以表示成:
        `simlarity = IDF score * TF score * fieldNorms`
    * 代码实现
        *  [文本挖掘（二）python 基于scikit-learn计算TF-IDF](https://cloud.tencent.com/developer/article/1800081)
        *  [使用scikit-learn tfidf计算词语权重](https://blog.csdn.net/levy_cui/article/details/77962768)
        *  [sklearn-TfidfVectorizer 计算过程详解](https://blog.csdn.net/m0_37991005/article/details/105074754)
        *  [L2正则处理](https://mathworld.wolfram.com/L2-Norm.html)
        *  文件
            `侧平石 砂浆 不符合`
            `积水 积水 砂浆`
        * 保存idf值
            ```python
            rom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer
            fr_dev_split = open(os.path.join(text_distribution_dir,'dev_split.txt'),'r')
            segments = [i.strip() for i in fr_dev_split.readlines()]
            
            # CountVectorizer是通过fit_transform函数将文本中的词语转换为词频矩阵，
            # 矩阵元素a[i][j] 表示j词在第i个文本下的词频。
            # 即各个词语出现的次数，通过get_feature_names()可看到所有文本的关键字，通过toarray()可看到词频矩阵的结果
            vectorizer = CountVectorizer()
            textvecot = vectorizer.fit_transform(segments)  # 得到文档向量化的矩阵内容
            
            # TfidfTransformer是统计vectorizer中每个词语的tf-idf权值
            transformer = TfidfTransformer(norm=None)
            tfidf = transformer.fit_transform(textvecot)  # 传入得到的字符串数组，得到tf-idf矩阵
            weight = tfidf.toarray() #将tf-idf矩阵抽取出来，元素a[i][j]表示j词在i类文本中的tf-idf权重
            
            # 获取词袋中所有文本关键词
            word = vectorizer.get_feature_names()
            print('关键词:',word)
            print('关键词数量:',len(word))
            # 查看词频结果
            df_word = pd.DataFrame(textvecot.toarray(), columns=vectorizer.get_feature_names())
            print('每条样本的单词词频: \n',df_word.head())
            # 查看计算的idf
            df_word_idf = pd.DataFrame(list(zip(vectorizer.get_feature_names(), transformer.idf_)), columns=['单词', 'idf'])
            print('每个单词的idf值: \n',df_word_idf.T.head())
            # 查看计算的tf-idf
            df_word_tfidf  = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names())  # 转换为数据框
            print(df_word_tfidf.iloc[0:4])
            df_word_tfidf['col_sum'] = df_word_tfidf.apply(lambda x:x.pow(2).sum(),axis=1)
            print(df_word_tfidf.iloc[0:4])
            ```
            ```python
            关键词: ['不符合', '侧平石', '砂浆', '积水']
            
            关键词数量: 4
            
            每条样本的单词词频: 
                不符合  侧平石  砂浆  积水
            0    1    1   1   0
            1    0    0   1   2
            
            每个单词的idf值: 
                        0        1   2        3
            单词       不符合      侧平石  砂浆       积水
            idf  1.40547  1.40547   1  1.40547
            
                    不符合       侧平石   砂浆       积水
            0  1.405465  1.405465  1.0  0.00000
            1  0.000000  0.000000  1.0  2.81093
            
                    不符合       侧平石   砂浆       积水   col_sum
            0  1.405465  1.405465  1.0  0.00000  4.950664
            1  0.000000  0.000000  1.0  2.81093  8.901329
            ```
        * 修改norm='l2'
            ```python
            关键词: ['不符合', '侧平石', '砂浆', '积水']
            关键词数量: 4
            
            每条样本的单词词频: 
                不符合  侧平石  砂浆  积水
            0    1    1   1   0
            1    0    0   1   2
            
            每个单词的idf值: 
                        0        1   2        3
            单词       不符合      侧平石  砂浆       积水
            idf  1.40547  1.40547   1  1.40547
            
                    不符合       侧平石        砂浆        积水
            0  0.631667  0.631667  0.449436  0.000000
            1  0.000000  0.000000  0.335176  0.942156
            
                    不符合       侧平石        砂浆        积水  col_sum
            0  0.631667  0.631667  0.449436  0.000000      1.0
            1  0.000000  0.000000  0.335176  0.942156      1.0
            ```



* BM25
    *  [BM25算法](https://zhuanlan.zhihu.com/p/79202151)
    *  [python根据BM25实现文本检索](https://www.jianshu.com/p/ff28004efb8a)
    *  [手动实现了Elasticsearch中的BM25](https://github.com/lsq960124/Inverted-index-BM25) 



## 参考

* [Learning to Rank](https://zhuanlan.zhihu.com/p/111636490)
* [【辩难】DSSM 损失函数是 Pointwise Loss 吗？](https://zhuanlan.zhihu.com/p/322065156)
