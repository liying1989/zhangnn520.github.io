---
description: 更新/总结/思考/todolist 会放在这里
---

# 🥳 Summary

[项目github](https://github.com/babytreemi/all-about-llm)

## ❤️ What‘s NEW？

* [ACL2023 Tutorial 关于检索增强型大语言模型以及应用](Tutorial\&Workshop/acl2023-retrieval-lm.md)
* [baichuan系列模型总结](tototolearn/openllm/bai-chuan-da-mo-xing.md)
* [Personality Traits in Large Language Models (Google DeepMind)](tototolearn/personality-traits-and-bias-in-llm/personality-traits-in-large-language-models.md)

## 👂🏻屯一些还要整理和入手的repo等

### Baichuan-13B 和 LLaMA2-13B 中文sft

https://huggingface.co/hiyouga/baichuan-13b-sft https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat

复现教程↑ https://zhuanlan.zhihu.com/p/645010851

相关git：

\[1] ChatGLM-6B, ChatGLM2-6B 微调：[https://github.com/hiyouga/ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning)&#x20;

\[2] LLaMA, LLaMA-2, BLOOM, Falcon, Baichuan, InternLM 微调: [https://github.com/hiyouga/LLaMA-Efficient-Tuning ](https://github.com/hiyouga/LLaMA-Efficient-Tuning)

\[3] 大模型知识编辑工具：[https://github.com/hiyouga/FastEdit](https://github.com/hiyouga/FastEdit)

### Firefly(流萤): 中文对话式大语言模型

repo： [https://github.com/yangjianxin1/Firefly](https://github.com/yangjianxin1/Firefly)

<mark style="background-color:red;">可以在16G显存上微调130亿的大模型，</mark>并且这套训练流程在Open LLM排行榜🤗上进行了验证，**比vicuna-13b-1.1略高0.2分，比llams-2-13b-chat略低0.5分**。

post：[https://mp.weixin.qq.com/s/KsbgRNTwXE86kCGTJhu0WQ](https://mp.weixin.qq.com/s/KsbgRNTwXE86kCGTJhu0WQ)

3D-LLM：将 3D 世界注入大型语言模型：[https://github.com/UMass-Foundation-Model/3D-LLM](https://github.com/UMass-Foundation-Model/3D-LLM)

### LIMA: Less Is More for Alignment, by Meta AI[&#xD; &#xD; ](https://github.com/yangjianxin1/Firefly)
