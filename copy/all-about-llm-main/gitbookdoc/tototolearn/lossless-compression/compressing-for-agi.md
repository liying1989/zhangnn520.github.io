---
description: Jack Rae
---

# ðŸ§  Compressing for AGI

ç›¸å…³è§£è¯»ï¼š[https://zhuanlan.zhihu.com/p/619511222](https://zhuanlan.zhihu.com/p/619511222)

åŽŸè§†é¢‘ï¼š[https://www.youtube.com/watch?v=dO4TPJkeaaU\&t=161s](https://www.youtube.com/watch?v=dO4TPJkeaaU\&t=161s)

## Theme of talk

* Think deeply about the <mark style="background-color:red;">training objective of foundation models</mark>
* <mark style="background-color:red;">What are we doing , why dose it make sense, and what are the limitations?</mark>

## Takeaways

* seek the minimum description length to solve perception
* Generative models are lossless compressors
* <mark style="background-color:green;">Large language models are state-of-the-art lossless text compressors(?!)</mark>
* Current limitation of thr apporach

## Minimum Description Length&#x20;

### ...and why it relates to perception

We want to deepest understanding of our observations

ones that generalize

<figure><img src="../../.gitbook/assets/img_v2_51408b25-2ef4-4a83-b03b-77137b87b63g.jpg" alt="" width="563"><figcaption></figcaption></figure>







