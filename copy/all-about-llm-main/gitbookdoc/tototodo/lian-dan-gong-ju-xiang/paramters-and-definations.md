# 🪐 Paramters and Definations

## Paramters

**temperature**&#x20;

温度控制着模型生成新词时的随机性程度。在基于概率的语言模型中，每个词都有一个与之对应的概率分布，温度通过增加或减少这些概率分布来影响模型生成单词的随机性。较高的温度会使得模型更倾向于选择概率较小的单词，从而生成更多的“冒险”文本。相反，较低的温度会强制模型更加倾向于选择概率最大的单词，从而生成更加可预测的文本。常见的温度值范围为0.5-1.5。

**topk**&#x20;

Top-k控制着模型生成新词时的选择数量。在生成每个新词时，模型会预测出若干个可能的单词，Top-k参数会限制模型只选择概率最大的前k个单词中的一个来作为生成的单词。Top-k可以帮助稳定生成过程，防止模型随意选择概率很小的单词。

**topp**

跟Top-k类似，Top-p也是控制着模型生成新词时的选择数量。在生成每个新词时，模型会预测出若干个可能的单词，Top-p参数会限制模型只选择概率最高的一些候选单词，直到这些候选单词的总概率达到一个阈值（如0.9或0.8）。Top-p可以帮助避免产生不符合语境的单词。

**max\_length**

为了避免生成无限长的文本，我们需要限制生成的文本长度。Max\_length参数控制生成文本的最大长度，一旦达到该长度，模型就会停止生成。Aquila系列模型的最大长度为2048个token。

...

## Definitions

**Adapter**

Adapter是一种轻量级的适应方法，它在预先训练的模型中添加少量可学习的参数，在保留其原始知识的同时实现高效的微调，从而实现高质量的响应并提高各种任务的性能。

**Alpaca**

Alpaca 是在 52K 指令跟踪演示上对 LLaMA 7B 模型进行微调的模型。它的行为在质量上与 OpenAI 的 text-davinci-003 相似，同时出人意料地小且易于/廉价复制（<600 美元）

**Attention**

注意力机制是大型语言模型中使用的一个组件，用于在处理过程中关注输入序列的特定部分，根据元素的相关性为元素分配不同的权重。它有助于捕获依赖性并提高模型生成上下文相关预测或输出的能力。

**Cosine Similarity**

余弦相似度是通过计算两个向量之间角度的余弦来确定两个向量之间相似度的度量。其范围为 -1 到 1，值越接近 1 表示相似度越高，值越接近 -1 表示不相似度。

**Embedding**

嵌入是连续向量空间中单词、句子或其他语言单元的密集、低维表示，捕获语义和上下文信息。它们是通过无监督方法学习的，并用于各种自然语言处理任务。

**GPTQ**

GPTQ 是一种基于近似二阶信息的一次性权重量化方法，能够有效压缩具有 1750 亿个参数的 GPT 模型，同时保持精度，允许单 GPU 执行并比 FP16 显着加速推理。

**Instruction Tuning**

在大型语言模型的背景下，指令调优是一种通过根据特定输入指令或示例优化其响应来微调模型的技术，从而提高其为给定提示或上下文生成相关且准确的输出的性能。

**LoRA**

LoRA 通过学习排序分解矩阵并结合冻结原始权重，实现大型语言模型中的参数减少。这显着减少了特定于任务的适应的存储需求，促进部署期间的高效任务切换，而不会引入推理延迟，并且与适配器、前缀调整和微调等其他适应方法相比，表现出卓越的性能。

[**Prefix tuning** ](https://arxiv.org/abs/2101.00190)

前缀调优是自然语言生成任务微调的轻量级替代方案。它保持语言模型参数冻结，但优化一个小的连续特定于任务的向量（称为前缀）。前缀调整从提示中汲取灵感，允许后续令牌关注此前缀，就好像它是“虚拟令牌”一样。

**Prompt Tuning**

Promp tuning是一种经济有效的方法，可以在不重新训练模型并更新其权重的情况下使人工智能基础模型适应新的下游任务。

**QLoRA**

QLoRA 是一种高效的微调方法，可显着降低内存需求，从而能够在单个 48GB GPU 上微调 650 亿个参数模型，而不会影响 16 位微调任务的性能。

**RLHF**

根据人类反馈进行强化学习（RLHF），也称为根据人类偏好进行强化学习，是一种利用人类反馈来训练“奖励模型”的技术。然后将该模型用作奖励函数，通过强化学习 (RL) 优化代理的策略。这是通过使用诸如近端策略优化之类的优化算法来实现的。

**Self-instruction**

Self-instruct是一种结合自监督学习和强化学习来训练大型语言模型的方法。它涉及使用模型自身的预测作为训练目标，使其能够从生成的数据中学习并提高其在特定任务上的性能。

**semantic search**

语义搜索是一种搜索技术，旨在理解用户查询和搜索内容背后的含义和意图。它超越了简单的关键字匹配，并考虑了查询和文档的上下文和语义，从而产生更准确和相关的搜索结果。

**Tokenization**

Tokenization是将文本分割成更小的单元以供 LLM 模型处理的过程。Byte Pair Encoding (BPE) 或 WordPiece 等子字算法用于将文本分割成更小的单元，从而捕获频繁和罕见的单词。

**Topic Modeling**

主题建模是一种统计技术，用于发现文档集合中的潜在主题或主题。它为单词分配概率分布，并根据单词共现模式识别主题，有助于理解文本语料库中存在的主要主题和结构。

**Transformer**

Transformer 是《Attention Is All You Need》论文中介绍的一种神经网络架构，广泛应用于大型语言模型中。它用注意力机制和自注意力层取代了传统的循环神经网络 (RNN)，用于捕获依赖性并改进顺序数据任务中的并行处理。

...

