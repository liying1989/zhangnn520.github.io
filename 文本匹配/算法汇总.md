---
sort: 1
---


# 算法汇总


* [算法开发手册](https://kg-nlp.github.io/Algorithm-Project-Manual/文本匹配/算法汇总.html)

* [个人知乎](https://www.zhihu.com/people/zhangyj-n)



## 知乎链接
* [算法框架-文本匹配-算法汇总（持续更新）](https://zhuanlan.zhihu.com/p/465584667)
* [算法框架-文本匹配-Match-Ignition](https://zhuanlan.zhihu.com/p/477772741)


## 负采样

* Mixed Negative Sampling for Learning Two-tower Neural Networks in Recommendations
    * 关键词: 负样本采样,双塔召回
    * 内容: 
        *  候选项太多,通常使用的是in_batch_negatives,完全按照item在正样本中的出现频次获取的正样本数据集,
        *  batch内采样的问题: 对于长尾item 无法在数据集中体现;还有item的分布无法调整
        *  论文提出的混合采样就是结合in_batch_negative和uniform采样,解决了负采样偏差问题SSB-Sample Selection Bias(同一batch使用使用相同的均匀采用的负样本)
        *  支持向量: 离超平面最近的样本点。svm最优化问题：各类样本点到超平面的距离最远。最大间隔超平面：第一，两类样本分隔在超平面的两侧；第二，两侧距离超平面最近的样本点(支持向量)到超平面的距离最大化。
    * [参考代码](https://github.com/agoodge/LUNAR/blob/main/utils.py)
    * [参考理解](https://zhuanlan.zhihu.com/p/533449018)
* Cross-Batch Negative Sampling for Training Two-Tower Recommenders
    * [参考理解](https://blog.csdn.net/codename_cys/article/details/121304870) 

* Embedding-based Retrieval in Facebook Search
    * 内容: 
        * 损失函数采用triplet loss,topk 的选择 = N/n (N是候选召回的数量,n是每条正样本采样的负例)
        * 困难负样本挖掘:从召回的top数据里找困难样本,再放回训练,这个top是一个范围可能是100-500
        * 训练检验:容易和困难样本比例(100:1,这个根据自己的任务尝试); ==先训练easy再训练hard或者相反==.
        * ==困难样本的挖掘使用之前自己寻找的难例分析方法==
        * 检索优化:
            * 量化,降低内存带宽和存储空间
            * IVF/BF/HNSW 提高检索速度和精度
            * 这些在milvus里面都有配置实现

* Support Vector Guided Softmax Loss for Face Recognition
    * 关键词: 度量函数 sv-softmax
    * 内容: 
        * 作者说的人脸问题:核心 特征可分性
        * 聚焦样本能提供什么信息:基于挖掘的策略(困难样本挖掘和focal loss)
        * 设计基于margin的损失函数,增加特征和在真实值投影上的margin,增加了可分性
        * 上述问题:困难样本随意性. 其他类别可分性不强
        * sv-softmax 把重点放在错误分类的点(支持向量)上:可以抑制难样本的随意性,同时汲取其他类别的可分能力,得到更好的可分特征
        * 直观分类:类内聚合,类间可分最优 
    * [参考代码](https://github.com/xiaoboCASIA/SV-X-Softmax)
    * [参考理解](https://zhuanlan.zhihu.com/p/83240213)

* 神经网络语言建模系列之三：重要性采样
    *  关键词:重要性采样
    *  内容: 解决训练效率问题,和上面的负采样没啥关系
    *  [参考理解](https://www.jianshu.com/p/9a162d99394d)

* Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations
    *  关键词: 采样偏置问题
    *  内容:
        * 流式数据处理中的采样概率修正,使用hash等技术提高检索效率,embedding归一化,加入超参温度系数τ来微调召回率和精确率
        * 想要解决的问题:
            * 从用户反馈中收集的训练数据通常很稀疏，导致对长尾内容的预测过拟合
            * 对于cold-start问题，推荐系统需要适应数据分布的变化来更好地推荐新内容（fresh content）
    * [参考理解](https://zhuanlan.zhihu.com/p/372550424)

* Sampled Softmax训练方法数学原理思考以及代码实现
    * [参考链接](https://zhuanlan.zhihu.com/p/143830417) 
    * [候选采样](https://zhuanlan.zhihu.com/p/143830417)

* YouTube深度学习推荐系统的十大工程问题
    * 根据上面的解读引入其他资料的学习
    * 关键词: 推荐转多分类,近邻搜索,日志处理
    * 内容:
        *  候选太多,使用softmax时效率太低,需要负采样(参考候选负采样)
        *  从百万候选向量中进行近邻搜索(现有的工具milvus已经解决)
        *  对新内容特征赋值
        *  训练集预处理,不采用原始用户日志,而是每个用户提取等数量样本,避免高活跃度用户对loss的影响
        *  不采用观看历史的时序特征,而是采用最近的浏览历史,这个出于经验,用户体验
        *  测试集不使用随机留一法,使用最近一次观看行为,防止出现标签泄露问题,(训练集已经采样了前几次的浏览结果,这里测试应该选最近的一次)
        *  优化目标采用每次曝光预期播放时间-符合商业目标
        *  对大量长尾的vidio直接用0向量代替,从工程上来说节约资源,节省推断时间,从模型角度讲,长尾因为数据量少准确性确实不好,扔掉也无所谓
        *  有些特征经过开方或平方处理,当做新特征输入-增加非线性,提升离线准确率
        *  排序模型使用加权逻辑回归作为输出层,服务时不直接用sigmoid 而是用指数形式预测结果( https://zhuanlan.zhihu.com/p/61827629)
            * 逻辑回归由来 在 《可解释性模型》中也有讲到
            * 对发生比Odd取自然对数
            * YouTube的结果就是发生比  
    * [参考理解](https://zhuanlan.zhihu.com/p/52504407)

* 负采样的几篇总结
    * 内容:
        * 基于启发式 
            *  曝光未点击的样本,当做负样本(batch内负样本)
            *  随机负采样(除去和样本有直接联系的数据,业务库中的其他标签)
            *  基于流行度负采样,越流行应该被推荐,但没有交互,所以是大概率负样本(对很相似的样本当做负样本)
            *  基于业务理解的负样本(业务标注,校核后的负样本)
            *  前一轮模型预测错误的(看难例分析,可以当做负样本,大多数已经工程化实现了)
                * 动态采样 DNS 
                    * 每轮模型将预测分数高的负例拿到下一轮做训练 
                    * 会存在伪负例问题,和强负例难以区分
                * 基于GAN的负采样
                    * 运用对抗思想采样使模型损失函数值增大的负例
                    * 时间开销大
                * [SRNS](https://zhuanlan.zhihu.com/p/344630367)
                    * 统计学特征作为先验知识对伪负例和强负例进行区分
                    * 采样去偏和采样效率优化
        * 根据采样效率
            * inbatch
            * uniform
            * mns
            * cbns
    * [负采样和Google双塔](https://www.cnblogs.com/charlesblc/p/16220126.html)
    * [一文读懂推荐系统负采样](https://zhuanlan.zhihu.com/p/387378387)
    * [热度降权](https://zhuanlan.zhihu.com/p/574752588)
    
## 综合参考

* [推荐系统中的深度匹配模型](https://zhuanlan.zhihu.com/p/101136699?utm_source=zhihu)
* [搜索中的深度匹配模型](https://zhuanlan.zhihu.com/p/113244063)
* [搜索中的深度匹配模型（下）](https://zhuanlan.zhihu.com/p/118183738)